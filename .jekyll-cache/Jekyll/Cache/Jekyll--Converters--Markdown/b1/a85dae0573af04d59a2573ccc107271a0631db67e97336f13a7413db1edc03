I"9<p>Regression analysis refers to a set of techniques for estimating relationships among variables. This post introduces <strong>linear regression</strong> augmented by <strong>basis functions</strong> to enable non-linear adaptation, which lies at the heart of supervised learning, as will be apparent when we turn to classification. Thus, a thorough understanding of this model will be hugely beneficial. We’ll go through 2 derivations of the optimal parameters namely the method of <strong>ordinary least squares (OLS)</strong>, which we briefly looked at in <a href="http://localhost:4000/bsmalea-notes-1a">notes 1a</a>, and <strong>maximum likelihood estimation (MLE)</strong>. We’ll also dabble with some Python throughout the post.</p>

<h3 id="setup-and-objective">Setup and objective</h3>
<p>Given a training dataset of $N$ input variables $\mathbf{x} \in \mathbb{R}^D$ with corresponding target variables $t \in \mathbb{R}$, the objective of regression is to construct a function $h(\mathbf{x})$ that yields prediction values of $t$ for new values of $\mathbf{x}$.</p>

<p>The simplest linear model for regression is just known as <em>linear regression</em>, where the predictions are generated by</p>

\[h\left(\mathbf{x},\mathbf{w} \right) = w_0 + w_1 x_1 + \dots + w_D x_D = w_0 + \sum_{i=1}^D w_i x_i. \quad \quad (1)\]
:ET