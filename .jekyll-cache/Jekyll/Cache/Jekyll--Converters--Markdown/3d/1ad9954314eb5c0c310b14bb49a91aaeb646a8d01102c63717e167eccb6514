I"¦<p>Regression analysis refers to a set of techniques for estimating relationships among variables. This post introduces <strong>linear regression</strong> augmented by <strong>basis functions</strong> to enable non-linear adaptation. Linear regression lies at the heart of supervised learning, as will be apparent when we turn to classification. Thus, a thorough understanding of this model will be hugely beneficial. Weâ€™ll go through two derivations of the optimal parameters namely the method of <strong>ordinary least squares (OLS)</strong>, which we briefly looked at in <a href="http://localhost:4000/bsmalea-notes-1a">notes 1a</a>, and <strong>maximum likelihood estimation (MLE)</strong>. Weâ€™ll also dabble with some Python throughout the post.</p>

<h3 id="setup-and-objective">Setup and objective</h3>
<p>Given a training dataset of $N$ input variables $\mathbf{x} \in \mathbb{R}^D$ with corresponding target variables $t \in \mathbb{R}$, the objective of regression is to construct a function $h(\mathbf{x})$ that yields prediction values of $t$ for new values of $\mathbf{x}$.</p>

<p>The simplest linear model for regression is just known as <em>linear regression</em>, where the predictions are generated by</p>

<div class="mathblock"><script type="math/tex; mode=display">
h\left(\mathbf{x},\mathbf{w} \right) = w_0 + w_1 x_1 + \dots + w_D x_D = w_0 + \sum_{i=1}^D w_i x_i. \quad \quad (1)
</script></div>

<p>The first term $w_0$ is commonly called the <em>intercept</em> or <em>bias</em> parameter, and allows $h$ to adapt to a fixed offset in the data - weâ€™ll show exactly what this means later in this post. If we introduce a $1$ as the first element of each $\mathbf{x}$, we can rewrite $(1)$ with vector notation, i.e. if we define $\mathbf{x} = \left( 1, x_1, \dots , x_D  \right)^\intercal$, we can rewrite $(1)$ as</p>

<script type="math/tex; mode=display">h(\mathbf{x},\mathbf{w}) = \mathbf{w}^\intercal \mathbf{x},</script>

:ET