I"™<p>A friend of mine recently asked me about word embeddings and similarity. I remember, I learned that the typical way of calculating the similarity between a pair of word embeddings is to take the cosine of the angle between their vectors. This measure of similarity makes sense due to the way that these word embeddings are commonly constructed, where each dimension is supposed to represent some sort of semantic meaning<span class="marginnote">These word embedding techniques have obvious flaws, such as words that are spelled the same way but have different meanings (called <a href="https://en.wikipedia.org/wiki/Homograph">homographs</a>), or sarcasm which often times is saying one thing but meaning the opposite.</span>. Yet, my friend asked if you could calculate the correlation between word embeddings as an alternative to cosine similarity, and it turns out that itâ€™s almost the exact same thing.</p>

<p><a href="https://www.aclweb.org/anthology/N19-1100/">Zhelezniak et al. (2019)</a> explains this well. Given a vocabulary of $N$ words $\mathcal{V} = \{ w_1, \dots, w_N \}$ with a corresponding word embedding matrix $\mathbf{W} \in \mathbb{R}^{N \times D}$, each row in $\mathbf{W}$ corresponds to a word. Considering a pair of these, we can calcuate their Pearson correlation coefficient (PCC). Let $(\mathbf{x}, \mathbf{y}) = \{ (x_1, y_1), \dots, (x_D, y_D) \}$ denote this pair, and we can compute the PCC as</p>

\[r_{xy} = \frac{ \sum_{i=1}^D (x_i - \bar{x})(y_i - \bar{y}) }{ \sqrt{\sum_{i=1}^D (x_i - \bar{x})^2} \sqrt{\sum_{i=1}^D (y_i - \bar{y})^2} },\]

<p>where $\bar{x} = \frac{1}{D} \sum_{i=1}^D x_i$ is the sample mean; and analagously to $\bar{y}$.</p>

:ET