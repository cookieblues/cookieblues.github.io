I"#<p>As mentioned in <a href="http://localhost:4000/guides/2021/03/08/bsmalea-notes-1a">notes 1a</a>, in classification the possible values for the target variables are discrete, and we call these possible values “classes”. In <a href="http://localhost:4000/guides/2021/03/22/bsmalea-notes-2">notes 2</a> we went through regression, which in short refers to constructing a function $h( \mathbf{x} )$ from a dataset $\mathbf{X} = \left( (\mathbf{x}_1, t_1), \dots, (\mathbf{x}_N, t_N) \right)$ that yields prediction values $t$ for new values of $\mathbf{x}$. The objective in classification is the same, except the values of $t$ are discrete.</p>

<p>We are going to cover 3 different approaches or types of classifiers:</p>

<ul>
  <li><strong>Generative classifiers</strong> that model the joint probability distribution of the input and target variables $p(\mathbf{x}, t)$.</li>
  <li><strong>Discriminative classifiers</strong> that model the conditional probability distribution of the target given an input variable $p(t | \mathbf{x})$.</li>
  <li><strong>Distribution-free classifiers</strong> that do not use a probability model but directly assign input to target variables.</li>
</ul>

<p>A quick disclaimer for this topic: <strong>the terminology will be very confusing</strong>, but we’ll deal with that when we cross those bridges.</p>

<h2 id="generative-vs-discriminative">Generative vs discriminative</h2>
<p>Here’s the list of classifiers that we will go over: for <strong>generative classifiers</strong> it’s <strong>quadratic discriminant analysis (QDA)</strong>, <strong>linear discriminant analysis (LDA)</strong>, and (Gaussian) <strong>naive Bayes</strong>, which are all special cases of the same model; for <strong>discriminative classiferis</strong> it’s <strong>logistic regression</strong>; and for <strong>distribution-free classifiers</strong> we will take a look at the <strong>perceptron</strong> but also the <strong>support vector machine (SVM)</strong>.</p>

<p>So, they all do the same thing (classification). Which one is the best? Which one should you use? Well, let’s recall <a href="http://localhost:4000/guides/2021/03/11/bsmalea-notes-1b/">the “no free lunch” theorem</a>, which broadly states that there isn’t one model that is always better than another. It always depends on your data. That being said, there are some things we can generally say about generative and discriminative classifiers. Ng and Jordan (2002) found that repeating the experiment of applying naive Bayes and logistic regression on binary classification tasks, naive Bayes (generative) performed better with less data, but logistic regression tended to perform better in general<span class="sidenote-number"></span><span class="sidenote">Andrew Y. Ng and Michael I. Jordan, “On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes,” 2001.</span>. However, Ulusoy and Bishop (2006) notes that this is only the case, when the data follow the assumptions of the generative model<span class="sidenote-number"></span><span class="sidenote">Ilkay Ulusoy and Christopher Bishop, “Comparison of Generative and Discriminative Techniques for Object Detection and Classification,” 2006.</span>, which means that logistic regression (discriminative) is generally better than naive Bayes (generative) - and that is the general consensus.</p>

<p>It cannot be stressed enough though that is not always the case, and you should not disregard generative models. As an example, generative adversarial networks (GANs) are generative models that have proved extremely useful in a variety of tasks.</p>

<p>What is best? Generative vs discriminative</p>

<p>Decision boundaries</p>

<h2 id="multivariate-gaussian-distribution">Multivariate Gaussian distribution</h2>

<h2 id="bayes-theorem">Bayes’ theorem</h2>
<p>If you haven’t read the <a href="http://localhost:4000/guides/2021/03/15/bsmalea-notes-1c/">post on frequentism and Bayesianism</a>, then here’s a quick recap on Bayes’ theorem.</p>

<p>Class conditional distribution.</p>

<p>Bayes theorem.</p>
:ET