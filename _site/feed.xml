<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <description>Personal blog</description>
    <link>http://localhost:4000</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>How to do data science without machine learning</title>
        <description>&lt;p&gt;I recently participated in a challenge orchestrated by &lt;a href=&quot;https://omdena.com/&quot;&gt;Omdena&lt;/a&gt;, which (if you don’t already know) is an international hub for AI enthusiasts that want to leverage AI solutions to solve some of humanity’s problems. The challenge was posed by a Nigerian NGO called RA365, and if you want to read more about it, Omdena asked me to write a small article for their blog regarding some work I did in the challenge. I didn’t really do much AI, so I decided to write about just that!&lt;/p&gt;

&lt;p&gt;You can find the article by clicking &lt;a href=&quot;https://medium.com/omdena/ai-in-nigeria-doing-data-science-for-good-without-machine-learning-6f7b1856d813&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Fri, 31 Jan 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000//how-to-do-data-science-without-machine-learning/</link>
        <guid isPermaLink="true">http://localhost:4000//how-to-do-data-science-without-machine-learning/</guid>
      </item>
    
      <item>
        <title>BSMALEA, notes 2: Regression</title>
        <description>&lt;p&gt;Regression analysis refers to a set of techniques for estimating relationships among variables. This post introduces &lt;strong&gt;linear regression&lt;/strong&gt; augmented by &lt;strong&gt;basis functions&lt;/strong&gt; to enable non-linear adaptation, which lies at the heart of supervised learning, as will be apparent when we turn to classification. Thus, a thorough understanding of this model will be hugely beneficial. We’ll go through 2 derivations of the optimal parameters namely the method of &lt;strong&gt;ordinary least squares (OLS)&lt;/strong&gt;, which we briefly looked at in &lt;a href=&quot;http://localhost:4000/bsmalea-notes-1a&quot;&gt;notes 1a&lt;/a&gt;, and &lt;strong&gt;maximum likelihood estimation (MLE)&lt;/strong&gt;. We’ll also dabble with some Python throughout the post.&lt;/p&gt;

&lt;h3 id=&quot;setup-and-objective&quot;&gt;Setup and objective&lt;/h3&gt;
&lt;p&gt;Given a training dataset of $N$ input variables $\mathbf{x} \in \mathbb{R}^D$ with corresponding target variables $t \in \mathbb{R}$, the objective of regression is to construct a function $h(\mathbf{x})$ that yields prediction values of $t$ for new values of $\mathbf{x}$.&lt;/p&gt;

&lt;p&gt;The simplest linear model for regression is just known as &lt;em&gt;linear regression&lt;/em&gt;, where the predictions are generated by&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;h\left(\mathbf{x},\mathbf{w} \right) = w_0 + w_1 x_1 + \dots + w_D x_D = w_0 + \sum_{i=1}^D w_i x_i. \quad \quad (1)&lt;/script&gt;
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;The first term $w_0$ is commonly called the &lt;strong&gt;intercept&lt;/strong&gt; or &lt;strong&gt;bias&lt;/strong&gt; parameter, and allows $h$ to adapt to a fixed offset in the data&lt;span class=&quot;sidenote-number&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sidenote&quot;&gt;We’ll show exactly what this means later in this post.&lt;/span&gt;. If we introduce a $1$ as the first element of each $\mathbf{x}$, we can rewrite $(1)$ with vector notation, i.e. if we define $\mathbf{x} = \left( 1, x_1, \dots , x_D  \right)^\intercal$, we can rewrite $(1)$ as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(\mathbf{x},\mathbf{w}) = \mathbf{w}^\intercal \mathbf{x},&lt;/script&gt;

&lt;p&gt;where $\mathbf{w} = \left(w_0, \dots, w_D \right)^\intercal$.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;http://localhost:4000/bsmalea-notes-1a&quot;&gt;notes 1a&lt;/a&gt; we went over polynomial regression with a $1$-dimensional input variable $x \in \mathbb{R}$. Now we’re just doing linear regression (not polynomial), but we’re allowing our input variable to be $D$-dimensional $\mathbf{x} \in \mathbb{R}^D$, hence it becomes a vector instead of a scalar. For the sake of visualization, however, let’s stick to the same example dataset:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Since our input variables are $1$-dimensional, we’ll have 2 parameters: $w_1$ and $w_0$, but to make sure we also find the bias parameter, we have to introduce a column of $1$s in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt;, like we defined $\mathbf{x} = \left( 1, x_1, \dots , x_D  \right)^\intercal$. We can do this with the following piece of code&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;column_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;derivation-and-training&quot;&gt;Derivation and training&lt;/h3&gt;
&lt;p&gt;So, how do we train the model? We’ll look at 2 different approaches of deriving the method of training this model. Recall that &lt;strong&gt;training&lt;/strong&gt; (or &lt;strong&gt;learning&lt;/strong&gt;) refers to &lt;strong&gt;the process of estimating the parameters&lt;/strong&gt; of our model, so when we ask how to train the model, it’s the same as asking how to estimate the values of $\mathbf{w}$.&lt;/p&gt;

&lt;h4 id=&quot;ordinary-least-squares&quot;&gt;Ordinary least squares&lt;/h4&gt;
&lt;p&gt;Like we did in &lt;a href=&quot;http://localhost:4000/bsmalea-notes-1a&quot;&gt;notes 1a&lt;/a&gt;, we defined an &lt;strong&gt;objective function&lt;/strong&gt; that calculated a measure of the performance of our model in terms of an error, and then we minimized this error with respect to our parameters. This means we would find the parameters that would result in the least error. We’ll use the same objective function as in &lt;a href=&quot;http://localhost:4000/bsmalea-notes-1a&quot;&gt;notes 1a&lt;/a&gt;, the sum of squared errors (SSE), defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
E(\mathbf{w}) &amp;= \sum_{n=1}^N \left( t_n - h \left( \mathbf{x}_n, \mathbf{w} \right) \right)^2 \\
&amp;= \sum_{n=1}^N \left( t_n - \mathbf{w}^\intercal \mathbf{x}_n \right)^2, \quad \quad (2)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;and we want to find values for $\mathbf{w}$ that minimizes $E$&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt; To recap: we want to find the parameter values $\hat{\mathbf{w}}$ that minimize our objective function, which is defined as the sum of the squared differences between $h(\mathbf{x}_i,\mathbf{w})$ and $t_i$.&lt;/span&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\hat{\mathbf{w}} = \underset{\mathbf{w}}{\arg\min} E(\mathbf{w}).
\end{aligned}&lt;/script&gt;

&lt;p&gt;Note that $(2)$ is a quadratic function of the parameters $\mathbf{w}$. Its partial derivatives with respect to $\mathbf{w}$ will therefore be linear in $\mathbf{w}$, which means there is a unique minimum. This is a property of &lt;strong&gt;convex&lt;/strong&gt; functions.
&lt;!-- NOTE ON CONVEX --&gt;&lt;/p&gt;

&lt;p&gt;If we evaluate the SSE for values of $w_0$ and $w_1$ in a grid, then we can illustrate that our objective function has a unique minimum with a contour plot. This is shown below.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;http://localhost:4000/extra/bsmalea-notes-2/weights.svg&quot; /&gt;
    &lt;span class=&quot;marginnote&quot;&gt;The cross is located at the minimum of our objective function - this coordinate corresponds to the values of $w_0$ (x-axis) and $w_1$ (y-axis) that produces the smallest SSE for our dataset. The contour lines show some boundaries for the SSE; we can see that the optimal values of $w_0$ and $w_1$ lie within a boundary of value $19$, meaning that the minimum value of SSE is less than $19$.&lt;/span&gt;
&lt;/figure&gt;

&lt;p&gt;So, how do we find the minimum of $E$? Recall from &lt;a href=&quot;http://localhost:4000/bslialo-notes-9b&quot;&gt;the notes about extrema&lt;/a&gt; that we find the minimum of a function by taking the derivative, setting the derivative equal to 0, and solving for the function variable. In our case, we have to take all the partial derivates of $E$ with respect to $w_0, \dots, w_{D}$ and set it equal to 0. Remember that all the partial derivatives of $E$ gives us the gradient $\nabla E$. To ease notation, let all our input variables be denoted by $\mathbf{X}$ with $N$ rows (one for each input variable) and $D+1$ columns (one for each feature plus one for the bias) defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{X} = \begin{pmatrix}
1 &amp; X_{11} &amp; \cdots &amp; X_{1D} \\
1 &amp; X_{21} &amp; \cdots &amp; X_{2D} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; X_{N1} &amp; \cdots &amp; X_{ND}
\end{pmatrix}, %]]&gt;&lt;/script&gt;

&lt;p&gt;and let $\textbf{\textsf{t}} = \left( t_1, \dots, t_N \right)^\intercal$ denote all our target variables. We can now rewrite $(2)$ as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
E(\mathbf{w}) &amp;= \sum_{n=1}^N \left( t_n - \mathbf{w}^\intercal \mathbf{x}_n \right)^2 \\
&amp;= (t_1 - \mathbf{w}^\intercal \mathbf{x}_1) (t_1 - \mathbf{w}^\intercal \mathbf{x}_1) + \cdots + (t_N - \mathbf{w}^\intercal \mathbf{x}_N) (t_N - \mathbf{w}^\intercal \mathbf{x}_N) \\
&amp;= \left( \textbf{\textsf{t}} - \mathbf{X}\mathbf{w} \right)^\intercal \left( \textbf{\textsf{t}} - \mathbf{X}\mathbf{w} \right) \\
&amp;= \left( \textbf{\textsf{t}}^\intercal - \left( \mathbf{X}\mathbf{w} \right)^\intercal \right) \left( \textbf{\textsf{t}} - \mathbf{X}\mathbf{w} \right) \\
&amp;= \textbf{\textsf{t}}^\intercal \textbf{\textsf{t}} - \textbf{\textsf{t}}^\intercal \mathbf{X}\mathbf{w} - \left( \mathbf{X}\mathbf{w} \right)^\intercal \textbf{\textsf{t}} + \left( \mathbf{X}\mathbf{w} \right)^\intercal \mathbf{X}\mathbf{w} \\
&amp;= \left( \mathbf{X}\mathbf{w} \right)^\intercal \mathbf{X}\mathbf{w} - 2 \left( \mathbf{X}\mathbf{w} \right)^\intercal \textbf{\textsf{t}} + \textbf{\textsf{t}}^\intercal \textbf{\textsf{t}}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;If we now take the derivative with respect to $\mathbf{w}$, we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\nabla E(\mathbf{w}) &amp;= \frac{\partial}{\partial \mathbf{w}} \left( \left( \mathbf{X}\mathbf{w} \right)^\intercal \mathbf{X}\mathbf{w} - 2 \left( \mathbf{X}\mathbf{w} \right)^\intercal \textbf{\textsf{t}} + \textbf{\textsf{t}}^\intercal \textbf{\textsf{t}} \right) \\
&amp;= 2 \mathbf{X}^\intercal \mathbf{X}\mathbf{w} - 2 \mathbf{X}^\intercal \textbf{\textsf{t}},
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;and setting this result equal to 0 lets us solve for $\mathbf{w}$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
2 \mathbf{X}^\intercal \mathbf{X}\mathbf{w} - 2 \mathbf{X}^\intercal \textbf{\textsf{t}}
&amp;= 0 \\
\mathbf{X}^\intercal \mathbf{X} \mathbf{w}
&amp;= \mathbf{X}^\intercal \textbf{\textsf{t}}\\
\mathbf{w}
&amp;= \left( \mathbf{X}^\intercal \mathbf{X} \right)^{-1} \mathbf{X}^\intercal \textbf{\textsf{t}},
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;which are our estimated values for the parameters&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt;To recap once again: $\left( \mathbf{X}^\intercal \mathbf{X} \right)^{-1} \mathbf{X}^\intercal \textbf{\textsf{t}}$ are the values of $\mathbf{w}$ that minimizes our SSE objective function defined in $(2)$.&lt;/span&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mathbf{w}} = \underset{\mathbf{w}}{\arg\min} E(\mathbf{w}) = \left( \mathbf{X}^\intercal \mathbf{X} \right)^{-1} \mathbf{X}^\intercal \textbf{\textsf{t}}.&lt;/script&gt;

&lt;h4 id=&quot;maximum-likelihood&quot;&gt;Maximum likelihood&lt;/h4&gt;
&lt;p&gt;Picking the SSE as the objective function might seem a bit arbitrary though - for example why not just go with the sum of the errors? Why do we have to square them? To show why this is a good choice, and why the solution makes sense, we are going to derive the same solution from a probabilistic perspective using &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&quot;&gt;maximum likelihood estimation (MLE)&lt;/a&gt;. To do this, we assume that the target variable $t$ is given by our function $h(\mathbf{x}, \mathbf{w})$ with a bit of noise added:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;t = h \left( \mathbf{x},\mathbf{w} \right) + \epsilon,&lt;/script&gt;

&lt;p&gt;where $\epsilon \sim \mathcal{N} \left( 0,\alpha \right)$, i.e. $\epsilon$ is a Gaussian random variable with mean 0 and variance $\alpha$. This lets us say that given an input variable $\mathbf{x}$, the corresponding target value $t$ is normally distributed with mean $h(\mathbf{x}, \mathbf{w})$ and variance $\alpha$, i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(t | \mathbf{x}, \mathbf{w}, \alpha) = \mathcal{N} \left( t | h(\mathbf{x},\mathbf{w}), \alpha \right). \quad \quad (3)&lt;/script&gt;

&lt;!--
Let's take a moment to understand exactly, what we're doing. The image below illustrates what $(3)$ tells us: for 
--&gt;

&lt;p&gt;We can now use the entire dataset, $\mathbf{X}$ and $\textbf{\textsf{t}}$, to write up the likelihood function by making the assumption that our data points are drawn independently from $(3)$. The likelihood function then becomes the product of $(3)$ for all our input and target variable pairs, and is a function of $\mathbf{w}$ and $\alpha$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\textbf{\textsf{t}} | \mathbf{X}, \mathbf{w}, \alpha) =
\prod_{i=1}^N \mathcal{N} \left( t_i | h(\mathbf{x}_i,\mathbf{w}), \alpha \right). \quad \quad (4)&lt;/script&gt;

&lt;p&gt;Now we want to maximize the likelihood, which means we want to determine the values of our parameters $\mathbf{w}$ and $\alpha$ that maximizes $(4)$. This might seem dauntingly difficult, but we can make it simpler with a handy trick.&lt;span class=&quot;marginnote&quot;&gt;Taking the log of the likelihood not only simplifies the math, but it helps computationally as well, since the product of many probabilities usually causes &lt;a href=&quot;https://en.wikipedia.org/wiki/Arithmetic_underflow&quot;&gt;underflow&lt;/a&gt;, whereas the sum of logs doesn’t.&lt;/span&gt; Since the logarithm is a monotonically increasing function, maximizing the log-likelihood is equivalent to maximizing the likelihood. Taking the log of the likelihood gives us&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\ln p(\textbf{\textsf{t}} | \mathbf{X}, \mathbf{w}, \alpha)
&amp;= \ln \left(
    \prod_{i=1}^N \mathcal{N} \left( t_i | h(\mathbf{x}_i,\mathbf{w}), \alpha \right)
\right) \\
&amp;= \sum_{i=1}^N \ln \left(
    \frac{1}{\sqrt{2 \pi \alpha}} \exp \left( -\frac{(t_n - \mathbf{w}^\intercal \mathbf{x}_i)^2}{2 \alpha} \right)
\right) \\
&amp;= N \ln \frac{1}{\sqrt{2 \pi \alpha}} - \sum_{i=1}^N \frac{(t_n - \mathbf{w}^\intercal \mathbf{x}_i)^2}{2 \alpha} \\
&amp;= - \frac{N}{2} \ln 2 \pi \alpha - \frac{1}{2 \alpha} \underbrace{\sum_{i=1}^N (t_n - \mathbf{w}^\intercal \mathbf{x}_i)^2}_{\text{SSE}}. \quad \quad (5)
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now it becomes evident why the SSE objective function is a good choice; the last term of $(5)$ is the only part dependent on $\mathbf{w}$ and is the same as SSE. Since the first term does not depend on $\mathbf{w}$, we can omit it, and since the maximum of the likelihood function with respect to $\mathbf{w}$ does not change by scaling with the positive constant $\frac{1}{2 \alpha}$, then we see &lt;strong&gt;maximizing the likelihood with respect to $\mathbf{w}$ is equivalent to minimizing the SSE objective function&lt;/strong&gt;. The maximum likelihood solution for $\mathbf{w}$ is therefore the same as in our previous derivation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{w}_{\text{ML}} = \left( \mathbf{X}^\intercal \mathbf{X} \right)^{-1} \mathbf{X}^\intercal \textbf{\textsf{t}}.&lt;/script&gt;

&lt;p&gt;We can use the result of the maximum likelihood solution for $\mathbf{w}$ to find the value of the noise variance $\alpha$. If we insert the maximum likelihood solution for $\mathbf{w}$ in the log-likelihood, take the derivative, and set it equal to 0, then we can solve for $\alpha$&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt;Note that we are in fact jointly maximizing the likelihood with respect to both $\mathbf{w}$ and $\alpha$, but because the maximization with respect to $\mathbf{w}$ is independent of $\alpha$, we start by finding the maximum likelihood solution for $\mathbf{w}$, and then use that result to find $\alpha$.&lt;/span&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{\partial}{\partial \alpha} \ln p(\textbf{\textsf{t}} | \mathbf{X}, \mathbf{w}_{\text{ML}}, \alpha)
&amp;= 0 \\
\frac{\partial}{\partial \alpha} \left(
    - \frac{N}{2} \ln 2 \pi \alpha - \frac{1}{2 \alpha} \sum_{i=1}^N (t_n - \mathbf{w}_{\text{ML}}^\intercal \mathbf{x}_i)^2
\right)
&amp;= 0 \\
-\frac{N}{2\alpha} + \frac{1}{2 \alpha^2} \sum_{i=1}^N (t_n - \mathbf{w}_{\text{ML}}^\intercal \mathbf{x}_i)^2
&amp;= 0 \\
\frac{1}{\alpha} \sum_{i=1}^N (t_n - \mathbf{w}_{\text{ML}}^\intercal \mathbf{x}_i)^2
&amp;= N \\
\alpha_{\text{ML}}
&amp;= \frac{1}{N} \sum_{i=1}^N (t_n - \mathbf{w}_{\text{ML}}^\intercal \mathbf{x}_i)^2.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;python-implementation&quot;&gt;Python implementation&lt;/h4&gt;
&lt;p&gt;In &lt;a href=&quot;http://localhost:4000/bsmalea-notes-1a&quot;&gt;notes 1a&lt;/a&gt; we implemented the OLS solution, but since we have a probabilistic model now, we make predictions that are probability distributions over $t$ instead of just point estimates. This is done by substituting the maximum likelihood solutions for $\mathbf{w}$ and $\alpha$ into $(3)$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(t | \mathbf{x}, \mathbf{w}_{\text{ML}}, \alpha_{\text{ML}}) = \mathcal{N} \left( t | h(\mathbf{x},\mathbf{w}_{\text{ML}}), \alpha_{\text{ML}} \right).&lt;/script&gt;

&lt;p&gt;We can find $\mathbf{w}$ and $\alpha$ with the following code snippet&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src=&quot;http://localhost:4000/extra/bsmalea-notes-2/prob_linreg.svg&quot; /&gt;
    &lt;span class=&quot;marginnote&quot;&gt;Plot of the line $w_0+w_1 x$ with our estimated values for $\mathbf{w}$ along with the uncertainty $\alpha$.&lt;/span&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;model-selection&quot;&gt;Model selection&lt;/h3&gt;
&lt;p&gt;You might be wondering what linear regression is so good for considering the image above, since it’s not doing well, but now we’re going to shine light on that by looking at ways to improve the simple linear regression model.&lt;/p&gt;

&lt;h4 id=&quot;basis-functions&quot;&gt;Basis functions&lt;/h4&gt;
&lt;p&gt;We call a model linear if it’s linear in the parameters &lt;em&gt;not&lt;/em&gt; in the input variables. However, $(1)$ is linear in both the parameters &lt;em&gt;and&lt;/em&gt; the input variables, which limits it from adapting to nonlinear relationships. We can augment the model by replacing the input variables with nonlinear &lt;strong&gt;basis functions&lt;/strong&gt; of the input variables&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
h(\mathbf{x},\mathbf{w}) &amp;= w_0 \phi_0(\mathbf{x}) + \cdots + w_{M-1} \phi_{M-1}(\mathbf{x}) \\
&amp;= \sum_{m=0}^{M-1} w_m \phi_m(\mathbf{x}) \\
&amp;= \mathbf{w}^\intercal \bm{\phi} (\mathbf{x}),
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt;Note that we had $D+1$ parameters in the simple linear regression model, but by augmenting it with basis functions, we now have $M$ parameters, which can be larger than $D$ if need be.&lt;/span&gt;
where we define $\bm{\phi}(\mathbf{x}) = \left( \phi_0 (\mathbf{x}), \dots, \phi_{M-1}(\mathbf{x}) \right)^\intercal$ and $\phi_0 ( {\mathbf{x}} ) = 1$ to keep the intercept $w_0$. By using nonlinear basis functions it is possible for $h$ to adapt to nonlinear relationships of $\mathbf{x}$, which we will see shortly - we call these models &lt;strong&gt;linear basis function models&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;We already looked at one example of basis functions in &lt;a href=&quot;http://localhost:4000/bsmalea-notes-1a&quot;&gt;notes 1a&lt;/a&gt;, where we augmented the simple linear regression model with basis functions of powers of $x$, i.e. $\phi_i (x) = x^i$. Another common basis function is the Gaussian&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi_i (\mathbf{x}) = \exp \left( - \gamma_i \| \bm{\mu}_i -\mathbf{x} \|_2^2  \right).&lt;/script&gt;

&lt;p&gt;Following the same derivation as before, we find the maximum likelihood solutions to be&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{w}_{\text{ML}} = \left( \mathbf{\Phi}^\intercal \mathbf{\Phi} \right)^{-1} \mathbf{\Phi}^\intercal \textbf{\textsf{t}}
\quad \text{and} \quad
\alpha_{\text{ML}} = \frac{1}{N} \sum_{i=1}^N (t_n - \mathbf{w}_{\text{ML}}^\intercal \bm{\phi}(\mathbf{x}_i))^2,&lt;/script&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{\Phi} = \begin{pmatrix}
\phi_0 (\mathbf{x}_1) &amp; \phi_1 (\mathbf{x}_1) &amp; \cdots &amp; \phi_{M-1} (\mathbf{x}_1) \\
\phi_0 (\mathbf{x}_2) &amp; \phi_1 (\mathbf{x}_2) &amp; \cdots &amp; \phi_{M-1} (\mathbf{x}_2) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
\phi_0 (\mathbf{x}_N) &amp; \phi_1 (\mathbf{x}_N) &amp; \cdots &amp; \phi_{M-1} (\mathbf{x}_N)
\end{pmatrix}. %]]&gt;&lt;/script&gt;

&lt;figure&gt;
    &lt;img src=&quot;http://localhost:4000/extra/bsmalea-notes-2/prob_linreg_basis.svg&quot; /&gt;
    &lt;span class=&quot;marginnote&quot;&gt;Illustration of the effect of using $M-1$ Gaussian basis functions plus the intercept.&lt;/span&gt;
&lt;/figure&gt;

&lt;p&gt;The Gaussian basis function for the plot above was implemented as below, where $\mu_i=\frac{i}{M}$ and $\gamma_i = 1$ for $i = 1, \dots, M-1$.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;gaussian_basis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;!-- MENTION POLYNOMIAL REGRESSION --&gt;

&lt;h4 id=&quot;regularization&quot;&gt;Regularization&lt;/h4&gt;
&lt;p&gt;We briefly ran into the concept of regularization in the &lt;a href=&quot;http://localhost:4000/bsmalea-notes-1c&quot;&gt;previous notes&lt;/a&gt;, which we described as a technique of preventing overfitting. If we look back at the objective function we defined earlier, augmented with basis functions, we can introduce a regularization term&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(\mathbf{w}) = \sum_{i=1}^N (t_n - \mathbf{w}^\intercal \bm{\phi}(\mathbf{x}_i))^2 + \underbrace{\lambda \sum_{j=0}^{M-1} | w_j |^q}_{\text{regularization}},&lt;/script&gt;

&lt;p&gt;where $q &amp;gt; 0$ denotes the type of regularization, and $\lambda$ controls the extent of regularization, i.e. how much do we care about the error from the data in relation to the regularization. The most common values of $q$ are $1$ and $2$, which are called $L_1$ regularization and $L_2$ regularization respectively. We call it &lt;strong&gt;lasso regression&lt;/strong&gt; when we use $L_1$ regularization, and &lt;strong&gt;ridge regression&lt;/strong&gt; when we use $L_2$ regularization.&lt;/p&gt;

&lt;p&gt;The objective function of ridge regression&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
E(\mathbf{w})
&amp;= \sum_{i=1}^N (t_n - \mathbf{w}^\intercal \bm{\phi}(\mathbf{x}_i))^2 + \lambda \sum_{j=0}^{M-1} | w_j |^2\\
&amp;= \sum_{i=1}^N (t_n - \mathbf{w}^\intercal \bm{\phi}(\mathbf{x}_i))^2 + \lambda \mathbf{w}^\intercal \mathbf{w} 
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;is especially convenient as it is a quadratic function of $\mathbf{w}$ and therefore has a unique global minimum. The solution to which is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mathbf{w}} = \left( \lambda \mathbf{I} + \mathbf{\Phi}^\intercal \mathbf{\Phi} \right)^{-1} \mathbf{\Phi}^\intercal \textbf{\textsf{t}},&lt;/script&gt;

&lt;p&gt;where $\alpha$ stays the same as without regularization, since the regularization term has no influence on it.&lt;/p&gt;

&lt;p&gt;When we introduce regularization, the process of model selection goes from finding the appropriate number of basis functions to finding the appropriate value for the regularization parameter $\lambda$.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;http://localhost:4000/extra/bsmalea-notes-2/prob_linreg_basis_regularization.svg&quot; /&gt;
    &lt;span class=&quot;marginnote&quot;&gt;Illustration of changing the value of the regularization parameter $\lambda$, while keeping the number of basis functions $M=8$ constant. Even though we overfitted earlier when $M=8$, our effective complexity is now controlled by the regularization instead, and the model will not overfit if $\lambda$ is large enough. Note also that as the regularization parameter is increased, the uncertainty increases as well.&lt;/span&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;We can find the parameters for a &lt;strong&gt;linear regression&lt;/strong&gt; through &lt;strong&gt;ordinary least squares&lt;/strong&gt; or &lt;strong&gt;maximum likelihood estimation&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Usually in &lt;strong&gt;linear regression&lt;/strong&gt; we have a scalar parameter that is not multiplied by the input called the &lt;strong&gt;intercept&lt;/strong&gt; or &lt;strong&gt;bias&lt;/strong&gt; denoted $w_0$.&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;process of estimating the values of the parameters&lt;/strong&gt; is called the &lt;strong&gt;training or learning&lt;/strong&gt; process.&lt;/li&gt;
  &lt;li&gt;Since the logarithm is a &lt;strong&gt;monotonically increasing&lt;/strong&gt; function, &lt;strong&gt;maximizing the likelihood function is the same as maximizing the log-likelihood function&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;What makes a model &lt;strong&gt;linear&lt;/strong&gt; is that it’s &lt;strong&gt;linear in the parameters&lt;/strong&gt; not the inputs.&lt;/li&gt;
  &lt;li&gt;We can augment linear regression with &lt;strong&gt;basis functions&lt;/strong&gt; yielding &lt;strong&gt;linear basis function models&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Polynomial regression&lt;/strong&gt; is a linear basis function model.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Regularization&lt;/strong&gt; is a technique of &lt;strong&gt;preventing overfitting&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;There are &lt;strong&gt;different kinds of regularization&lt;/strong&gt; in linear regression such as $L_1$ and $L_2$ regularization.&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 02 Dec 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000//bsmalea-notes-2/</link>
        <guid isPermaLink="true">http://localhost:4000//bsmalea-notes-2/</guid>
      </item>
    
      <item>
        <title>BSMALEA, notes 1b: Model selection and validation, the &quot;no free lunch&quot; theorem, and the curse of dimensionality</title>
        <description>&lt;p&gt;Now we know a bit about machine learning: it involves models. Machine learning attempts to model data in order to make predictions about the data. In the &lt;a href=&quot;http://localhost:4000/bsmalea-notes-1a&quot;&gt;previous post&lt;/a&gt; we dove into the inner functions of a model, and that is very much what machine learning is about. Yet, it’s only half of it, really. The other half has to do with the concept of prediction, and how we make sure our models predict well. This course doesn’t dabble that deep into this other half - but there are a few important topics in this regard that you should be aware of, as they pop up in machine learning all the time.&lt;/p&gt;

&lt;h3 id=&quot;model-selection-and-validation&quot;&gt;Model selection and validation&lt;/h3&gt;
&lt;p&gt;In the &lt;a href=&quot;http://localhost:4000/bsmalea-notes-1a&quot;&gt;previous post&lt;/a&gt;, we went over polynomial regression. In the Python implementation, we went with a 4th order polynomial. However, perhaps $M=4$ isn’t the ‘best’ choice - but what is the ‘best’ choice, and how do we find it?
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;Firstly, the order of our polynomial is set before the training process begins, and we call these special parameters in our model &lt;strong&gt;“hyperparameters”&lt;/strong&gt;. Secondly, the process of figuring out the values of these hyperparameters is called &lt;strong&gt;hyperparameter optimization&lt;/strong&gt; and is a part of &lt;strong&gt;model selection&lt;/strong&gt;. Thirdly, as mentioned in the &lt;a href=&quot;http://localhost:4000/bsmalea-notes-1a&quot;&gt;previous post&lt;/a&gt;, machine learning is mostly concerned with prediction, which means that we define the ‘best’ model as the one that &lt;strong&gt;generalizes&lt;/strong&gt; the best on future data, i.e. which model would perform the best on data it wasn’t trained on?&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt;Note that the evaluation metric can be different from the objective function. In fact, it often times is.&lt;/span&gt;
To begin with, we usually want to come up with some kind of &lt;strong&gt;evaluation metric&lt;/strong&gt;. Then we divide our training dataset into 3 parts: a &lt;strong&gt;training&lt;/strong&gt;, a &lt;strong&gt;validation&lt;/strong&gt; (sometimes called &lt;strong&gt;development&lt;/strong&gt;), and a &lt;strong&gt;test&lt;/strong&gt; dataset.&lt;span class=&quot;marginnote&quot;&gt;Commonly the new training dataset is $80\%$ of the original, and the validation and test datasets are $10\%$ each.&lt;/span&gt; Then we train our model on the training dataset, perform model selection on the validation dataset, and do a final evaluation of the model on the test dataset. This way, we can determine the model with the lowest &lt;strong&gt;generalization error&lt;/strong&gt;. The generalization error refers to the performance of the model on &lt;strong&gt;unseen data&lt;/strong&gt;, i.e. data that the model hasn’t been trained on.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/extra/bsmalea-notes-1b/model_selection_poly_reg.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s go back to the polynomial regression example. In the image above I’ve plotted the data points from the &lt;a href=&quot;http://localhost:4000/bsmalea-notes-1a&quot;&gt;previous post&lt;/a&gt; together with the true function and $4$ different estimated polynomials of $4$ different orders: $2$, $4$, $6$, and $8$. As we increase the order of the polynomial, we increase what we call the &lt;strong&gt;complexity&lt;/strong&gt; of our model, which roughly can be seen as correlated with the number of parameters in the model. So the more parameters our model has, roughly the more complex it is. As the order of the polynomial (the complexity of the model) increases, it begins approximating the data points better, until it perfectly goes through all the data points. Yet, if we perfectly match the data points in our training dataset, our model probably won’t generalize very well, because the data isn’t perfect; there’s always a bit of noise, which can also be seen above. When we end up fitting our model perfectly to out training dataset, which makes the model generalize poorly, we say that we are &lt;strong&gt;overfitting&lt;/strong&gt;. In the image above, when the order is set to $8$ ($M=8$), we’re definitely overfitting. Conversely, when $M=2$, we could argue that we are &lt;strong&gt;underfitting&lt;/strong&gt;, which means that the complexity of our model isn’t high enough to ‘capture the richness of variation’ in our data. In other words, it doesn’t pick up on the patterns in the data.&lt;/p&gt;

&lt;h4 id=&quot;cross-validation&quot;&gt;Cross-validation&lt;/h4&gt;
&lt;p&gt;If you have limited data, you might feel disadvantaged with the $80$-$10$-$10$ technique, because the size of the validation and test set is small, thereby not being a proper representation of your entire training set. In the extreme case of only $10$ data points, this would result in a validation and test set of size $1$, which is not exactly a great sample size! Instead, different techniques called &lt;strong&gt;cross-validation&lt;/strong&gt; can be applied. The most common is the &lt;strong&gt;k-fold cross-validation&lt;/strong&gt; technique, where you divide your dataset $\mathcal{D} = \{ \left( \mathbf{x}_1, t_1 \right), \dots, \left(\mathbf{x}_N, t_N \right) \}$ into $k$ distinct subsets. By choosing $1$ of the $k$ subsets to be the validation set, and the rest $k-1$ subsets to be the training set, we can repeat this process $k$ times by choosing a different subset to be the validation set every time. This makes it possible to repeat the training-validation process $k$ times, eventually going through the entire original training set as both training and validation set.&lt;/p&gt;

&lt;!--
INSERT ANIMATION FOR CROSS-VALIDATION
--&gt;

&lt;h4 id=&quot;python-implementation&quot;&gt;Python implementation&lt;/h4&gt;
&lt;p&gt;Following the example from &lt;a href=&quot;http://localhost:4000/bsmalea-notes-1a&quot;&gt;last post&lt;/a&gt; we can try and figure out the best order $M$ for our polynomial. We start be defining our evaluation metric; we will use the popular &lt;strong&gt;mean squared-error (MSE)&lt;/strong&gt;, which is very closely related to the sum of squared errors (SSE) function that we looked at briefly in the last post. The MSE is defined as the mean of the squared differences between our predictions and the true values, formally&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{MSE} = \frac{1}{N} \sum_{n=1}^N \left( t_n - h(x_n,\mathbf{w}) \right)^2,&lt;/script&gt;

&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt;The only difference between the mean squared-error and the sum of squared errors is that we’re now dividing by the number of data points.&lt;/span&gt;
where $N$ is the number of data points, $h$ is our polynomial, $\mathbf{w}=\left(w_0, \dots, w_M \right)^\intercal$ are the coefficients of our polynomial (the model parameters), and $(x_n, t_n)$ is an input-target variable pair. Below is a simple Python implementation of MSE that takes NumPy arrays as input. Make sure that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pred&lt;/code&gt; arrays are the same length - this could be done with an assertion if needed.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And below is an implementation of k-fold cross-validation. It yields the indices of the train and validation set for each fold. We only have to make sure that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_splits&lt;/code&gt; is not larger than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n_points&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;kfold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_points&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_splits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;split_sizes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_splits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_points&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_splits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;leftover&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_points&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_splits&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;split_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;leftover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_points&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;current&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;split_size&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;split_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;val_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;train_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_idx&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;current&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;split_size&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;the-no-free-lunch-theorem&quot;&gt;The “no free lunch” theorem&lt;/h3&gt;
&lt;p&gt;While machine learning tries to come up with the best models, in actuality we empirically choose the best model when confronted with a task&lt;span class=&quot;sidenote-number&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sidenote&quot;&gt;S. Raschka, “Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning,” 2018.&lt;/span&gt;. This is what model selection and validation, which we just learned about, does for us. A known theorem in machine learning (and optimization) is &lt;strong&gt;the “no free lunch” theorem&lt;/strong&gt;&lt;span class=&quot;sidenote-number&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sidenote&quot;&gt;D. H. Wolpert, “The Lack of A Priori Distinctions Between Learning Algorithms,” 1996.&lt;/span&gt;, which broadly says that there’s no universally best model. That is, you cannot say that one model is better than another model in all cases, e.g. mixture models are better than neural networks or vice-versa. This is why it’s important to learn about a plethora of models, so when you’re confronted with a task, you know not only to try one model and be content, if it’s doing alright or matches your expectation; there could be another model significantly outperforming, what you’re seeing.&lt;/p&gt;

&lt;h3 id=&quot;the-curse-of-dimensionality&quot;&gt;The curse of dimensionality&lt;/h3&gt;
&lt;p&gt;As mentioned in the beginning, this post is mainly about the issue of determining which models generalize the best. The “no free lunch” theorem tells us that we can never say that one model is the best, and model selection and validation gives us a framework to actually determine the best model for a specific task; the &lt;strong&gt;curse of dimensionality&lt;/strong&gt; is a common enemy in this determination, which is even inherent in our training data! The term was first coined by Richard E. Bellman in 1957&lt;span class=&quot;sidenote-number&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sidenote&quot;&gt;R. E. Bellman, “Dynamic Programming,” 1957.&lt;/span&gt; to refer to the intractability of certain algorithms in high dimensionality. To facilitate the understanding of the curse of dimensionality, we’ll go through an example of classification. As mentioned in the &lt;a href=&quot;http://localhost:4000/bsmalea-notes-1a&quot;&gt;previous post&lt;/a&gt;, classification is a supervised learning task, where we have to organize our data points into discrete groups that we call classes.&lt;/p&gt;

&lt;p&gt;So far we’ve been looking at polynomial regression with only a $1$-dimensional input variable $x$. However, in most practical cases, we’ll have to deal with data of high dimensionality, e.g. if humans were our observations, they could have multiple values describing them: height, weight, age, etc. In our example, we’ll have $10$ data points $N=10$, $2$ classes $C=2$, and each data point is $3$-dimensional $D=3$.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.33&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.88&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.74&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.54&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.62&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.79&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.07&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.31&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.83&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.24&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.47&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.47&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.82&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.70&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.51&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.76&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.51&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.71&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.92&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.59&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.78&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.43&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.53&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.53&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The code snippet above shows our training dataset; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X&lt;/code&gt; is our $10$ input variables of dimensionality $3$ (we have $3$ features), and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t&lt;/code&gt; is our target variables, which in this case corresponds to $2$ classes. We can also see that the first $5$ data points belong to class $0$ and the last $5$ to class $1$; we have an equal distribution between our classes. If we plot the points using only the first feature (the first column in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X&lt;/code&gt;), we get the plot underneath. A naive approach to classify the points would be to split the line into $5$ segments (each of length $0.2$), and then decide to classify all the points in this segment into class $0$ or $1$. In the image underneath I’ve coloured the segments after the classification I would make. With this naive approach we get $3$ mistakes.&lt;/p&gt;

&lt;p style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;http://localhost:4000/extra/bsmalea-notes-1b/one_dim_cod.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But let’s see if we can do better! Using the first $2$ features now gives us a grid (of $0.2$ by $0.2$ tiles) that we can now use for our naive classification model. As shown underneath, we can now classify the points such that we only make $1$ mistake.&lt;/p&gt;

&lt;p style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;http://localhost:4000/extra/bsmalea-notes-1b/two_dim_cod.svg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we use all $3$ features, we can classify all the points perfectly, which is illustrated underneath. This is because, we now have $0.2$ by $0.2$ by $0.2$ cubes. From this it might seem like using all $3$ features is better than just using $1$ or $2$, since we’re able to better classify our data points - but this is where the counterintuitive concept of the &lt;strong&gt;curse of dimensionality&lt;/strong&gt; comes in, and I tell you that it’s &lt;em&gt;not&lt;/em&gt; better to use all the features.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/extra/bsmalea-notes-1b/three_dim_cod.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The issue relates to the proportion of our data points compared to our classification sections; with $1$ feature we had $10$ points and $5$ sections, i.e. $\frac{10}{5}=2$ points per section, with $2$ features we had $\frac{10}{5 \times 5}=0.4$ points per section, and with $3$ features we had $\frac{10}{5 \times 5 \times 5}=0.08$ points per section. As we add more features, the available data points in our &lt;strong&gt;feature space&lt;/strong&gt; become exponentially sparser, which makes it easier to separate the data points. Yet, it’s not because of any pattern in the data, in actuality it’s just the nature of higher dimensional spaces. In fact, the data points I listed were randomly generated from a uniform distribution, so the ‘pattern’ we’re fitting to isn’t actually there at all - it’s a result of the increased dimensionality, which makes the available data points become sparser. Because of this inherent sparsity we end up overfitting, when we add more features to our data. Which means we need more data to avoid sparsity, and that’s the curse of dimensionality: as the number of features increase, our data become sparser, which results in overfitting, and we therefore need more data to avoid it.&lt;/p&gt;

&lt;p&gt;So how do we avoid getting cursed? Luckily, the &lt;strong&gt;blessing of non-uniformity&lt;/strong&gt;&lt;span class=&quot;sidenote-number&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sidenote&quot;&gt;P. Domingos, “A few useful things to know about machine learning,” Communications of the ACM, vol. 55, no. 10, pp. 78-87, 2012.&lt;/span&gt; comes to our rescue! In most practical scenarios our data isn’t spread out uniformly but is rather concentrated in some places, which nullifies the curse of dimensionality. But what if it really &lt;em&gt;is&lt;/em&gt; the curse of dimensionality? There’s not a right answer, as it really depends on the dataset, but there is a related &lt;a href=&quot;https://en.wikipedia.org/wiki/One_in_ten_rule&quot; target=&quot;_blank&quot;&gt;one in ten rule&lt;/a&gt; of thumb; for every model parameter (roughly feature) we want at least $10$ data points. Some better options fall under the topic of &lt;strong&gt;dimensionality reduction&lt;/strong&gt;, which we will look at later on in the course.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Hyperparameters&lt;/strong&gt; are the parameters in a model that &lt;strong&gt;are determined before training&lt;/strong&gt; the model.&lt;/li&gt;
  &lt;li&gt;Model selection refers to the proces of &lt;strong&gt;choosing the model that best generalizes&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Training and validation sets&lt;/strong&gt; are used to &lt;strong&gt;simulate unseen data&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Overfitting&lt;/strong&gt; happens when our model &lt;strong&gt;performs well on our training dataset but generalizes poorly&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Underfitting&lt;/strong&gt; happens when our model &lt;strong&gt;performs poorly on both our training dataset and unseen data&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;We can see if our model generalizes well with cross-validation techniques.&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;mean squared-error&lt;/strong&gt; or &lt;strong&gt;MSE&lt;/strong&gt; is a common evaluation metric.&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;“no free lunch”&lt;/strong&gt; theorem tells us that there is &lt;strong&gt;no best model&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;more features&lt;/strong&gt;, the &lt;strong&gt;higher risk of overfitting&lt;/strong&gt; is the curse of dimensionality in a nut shell.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 26 Nov 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000//bsmalea-notes-1b/</link>
        <guid isPermaLink="true">http://localhost:4000//bsmalea-notes-1b/</guid>
      </item>
    
      <item>
        <title>BSMALEA, notes 1a: What is machine learning?</title>
        <description>&lt;p&gt;It seems, most people derive their definition of machine learning from a quote from Arthur Lee Samuel in 1959: “Programming computers to learn from experience should eventually eliminate the need for much of this detailed programming effort.” The interpretation to take away from this is that “machine learning is the field of study that gives computers the ability to learn without being explicitly programmed.”&lt;/p&gt;

&lt;p&gt;While Arthur Lee Samuel first coined the term “machine learning” in 1959&lt;span class=&quot;sidenote-number&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sidenote&quot;&gt;R. Kohavi and F. Provost, “Glossary of terms,” Machine Learning, vol. 30, no. 2–3, pp. 271–274, 1998.&lt;/span&gt;, the methods applied in machine learning come way before, e.g. the method of least-squares was first published by Adrien-Marie Legendre in 1805&lt;span class=&quot;sidenote-number&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sidenote&quot;&gt;A. M. Legendre, “Nouvelles méthodes pour la détermination des orbites des comètes,” 1805.&lt;/span&gt;, and Bayes’ theorem, which is the cornerstone of Bayesian statistics that has taken off in the 21st century, was first underpinned by Thomas Bayes in 1763&lt;span class=&quot;sidenote-number&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sidenote&quot;&gt;T. Bayes and R. Price, “An essay towards solving a problem in the doctrine of chances,” in a letter to J. Canton, 1763.&lt;/span&gt;.&lt;/p&gt;

&lt;!---
INSERT TIMELINE
--&gt;

&lt;p&gt;Machine learning draws a lot of its methods from statistics, but there is a distinctive difference between the two areas: &lt;strong&gt;statistics is mainly concerned with estimation&lt;/strong&gt;, whereas &lt;strong&gt;machine learning is mainly concerned with prediction&lt;/strong&gt;. This distinction makes for great differences, as we will see soon enough.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h3 id=&quot;categories-of-machine-learning&quot;&gt;Categories of machine learning&lt;/h3&gt;
&lt;!--
WRITE OUT THAT CLASSIFICATION/REGRESSION ARE SUPERVISED AND CLUSTERING/DENSITY ESTIMATION ARE UNSUPERVISED
--&gt;
&lt;p&gt;There are many different machine learning methods that solve different tasks. This course presupposes two fundamental ones; &lt;strong&gt;supervised&lt;/strong&gt; learning and &lt;strong&gt;unsupervised&lt;/strong&gt; learning. These are further divided into two smaller categories as shown in the image underneath.&lt;/p&gt;

&lt;!---
INSERT ANIMATION
--&gt;
&lt;figure&gt;
    &lt;img src=&quot;http://localhost:4000/extra/bsmalea-notes-1a/categories_of_ml.svg&quot; /&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;supervised-learning&quot;&gt;Supervised learning&lt;/h4&gt;
&lt;!--
MORE EXPLICIT THAT THERE ARE TARGET VALUES
MAYBE MORE EXPLICIT THAT THERE ARE ONE TARGET FOR EACH INPUT
--&gt;
&lt;p&gt;Supervised learning refers to a subset of machine learning tasks, where we’re given a dataset $\mathcal{D} = \left\{ (\mathbf{x}_1,\mathbf{t}_1), \dots, (\mathbf{x}_N,\mathbf{t}_N) \right\}$ of $N$ input-output pairs, and our goal is to come up with a function $h$ from the inputs $\mathbf{x}$ to the outputs $\mathbf{t}$. In more layman’s terms: we are given a dataset with specific predetermined labels that we want to predict - hence the learning is &lt;em&gt;supervised&lt;/em&gt;, since we are handed some data that tells us, what we want to predict. Each input-output pair refers to observations, where we want to predict the output from the input. Each input variable $\mathbf{x}$ is a $D_1$-dimensional vector (or a scalar), representing the observation with numerical values; these are commonly called &lt;strong&gt;features&lt;/strong&gt; or &lt;strong&gt;attributes&lt;/strong&gt;. Likewise, each output or &lt;strong&gt;target&lt;/strong&gt; variable $\mathbf{t}$ is a $D_2$-dimensional vector (but most often just a scalar).&lt;/p&gt;

&lt;p&gt;In &lt;strong&gt;classification&lt;/strong&gt; the possible values for the target variables form a finite number of discrete categories $t \in \{ C_1, \dots, C_k \}$ commonly called &lt;strong&gt;classes&lt;/strong&gt;. An example of this could be trying to classify olive oil into geographical regions (our classes) based on various aspects (our features) of the olive oils&lt;span class=&quot;sidenote-number&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sidenote&quot;&gt;J. Gromadzka and W. Wardencki, “Trends in Edible Vegetable Oils Analysis. Part B. Application of Different Analytical Techniques,” 2011.&lt;/span&gt;. The features could be concentrations of acids in the olive oils, and the classes could be northern and southern France. Another classic example is recognizing handwritten digits&lt;span class=&quot;sidenote-number&quot;&gt;&lt;/span&gt;; given an image of $28 \times 28$ pixels,&lt;span class=&quot;sidenote&quot;&gt;Y. LeCun et al., “Gradient-based learning applied to document recognition,” 1998.&lt;/span&gt; we can represent each image as a $784$-dimensional vector, which will be our input variable, and our targets will be scalars from $0$ to $9$ each representing a distinct digit. A third example could be predicting whether students are in risk of dropping out or not. In this case, we have two classes (in risk and not in risk), which we can predict based on data about the student e.g. grades or attendance. This is actually something I implemented once!&lt;/p&gt;

&lt;p&gt;You might’ve heard of &lt;strong&gt;regression&lt;/strong&gt; before. It’s the same as classification, except the target variable now is continuous $\mathbf{t} \in \mathbb{R}^{D_2}$ where $D_2 \geq 1$. So, given an input variable, we want to predict some continuous target variable.
&lt;!-- EXAMPLES --&gt;&lt;/p&gt;

&lt;h4 id=&quot;unsupervised-learning&quot;&gt;Unsupervised learning&lt;/h4&gt;
&lt;p&gt;Another subset of machine learning tasks falls under unsupervised learning, where we’re only given a dataset $\mathcal{D} = \left\{ \mathbf{x}_1, \dots, \mathbf{x}_N \right\}$ of $N$ input variables. In contrast to supervised learning, we’re not told what we want to predict, i.e., we’re not given any target variables. The goal of unsupervised learning is then to find patterns in the data.&lt;/p&gt;

&lt;p&gt;The image above divides unsupervised learning into two subtasks; the first one being &lt;strong&gt;clustering&lt;/strong&gt;, which, as the name suggests, refers to the task of discovering ‘clusters’ in the data. &lt;span class=&quot;marginnote&quot;&gt;Notice that the definition of similarity can vary depending on what data you’re dealing with, e.g. disco balls and tennis balls are more similar in shape than compared to hockey pucks, but hockey pucks and tennis balls are more similar in their usage.&lt;/span&gt;We can define a cluster to be &lt;strong&gt;a group of observations that are more similar to each other than to observations in other clusters&lt;/strong&gt;.&lt;/p&gt;

&lt;!-- EXAMPLES --&gt;

&lt;!--
REWRITE, PERHAPS EXAMPLE FIRST
--&gt;
&lt;p&gt;The second subtask is &lt;strong&gt;density estimation&lt;/strong&gt;, which is the task of fitting probability density functions to the data. It’s important to note that density estimation is often done in conjunction to other tasks like classification, e.g. based on the given classes of our observations, we can use density estimation to find the distributions of each class and thereby (based on the class distributions) classify new observations.
&lt;!-- EXAMPLES --&gt;&lt;/p&gt;

&lt;h3 id=&quot;example-polynomial-regression&quot;&gt;Example: polynomial regression&lt;/h3&gt;
&lt;p&gt;Let’s go through an example of machine learning. This is also to get familiar with the machine learning vernacular. We’re going to implement a model called &lt;em&gt;polynomial regression&lt;/em&gt;, which is where we try and fit a polynomial to our data. Given a training dataset of $N$ input variables $x \in \mathbb{R}$ (notice we assume our input variables are one-dimensional) with corresponding target variables $t \in \mathbb{R}$, our objective is to fit a polynomial that yields values $\hat{t}$ of target variables for new values $\hat{x}$ of the input variable. We’ll do this by estimating the coefficients of the polynomial&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(x, \mathbf{w}) = w_0 + w_1 x + w_2 x^2 + \dots + w_M x^M = \sum_{m=0}^M w_m x^m, \quad \quad (1)&lt;/script&gt;

&lt;p&gt;which we refer to as the &lt;strong&gt;parameters&lt;/strong&gt; or &lt;strong&gt;weights&lt;/strong&gt; of our model. $M$ is the order of our polynomial, and $\mathbf{w} = \left( w_0, w_1, \dots, w_M \right)^\intercal$ denotes all our parameters, i.e. we have $M+1$ parameters for our $M$th order polynomial.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt;In the &lt;a href=&quot;http://localhost:4000/bsmalea-notes-1b&quot;&gt;next post&lt;/a&gt; we’ll discuss exactly what we mean by ‘best’ values.&lt;/span&gt;
Now, the objective is to estimate the ‘best’ values for our parameters. To do this, we define what is called an &lt;strong&gt;objective function&lt;/strong&gt; (also sometimes called &lt;strong&gt;error&lt;/strong&gt; or &lt;strong&gt;loss&lt;/strong&gt; function). We construct our objective function such that it outputs a value that tells us how our model is performing. For this task we define the objective function as the sum of the squared differences between the predictions of our polynomial given input variables and the corresponding target variables, i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(\mathbf{w}) = \sum_{n=1}^N \left( t_n - h(x_n, \mathbf{w}) \right)^2, \quad \quad (2)&lt;/script&gt;

&lt;p&gt;and if we substitute $h(x_n, \mathbf{w})$ with the sum on the right-hand side of $(1)$, we get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(\mathbf{w}) =  \sum_{n=1}^N \left( t_n - \sum_{m=0}^M w_m x^m \right)^2.&lt;/script&gt;

&lt;p&gt;Let’s take a minute to understand what $(2)$ is saying. The term on the right-hand side between the paranthesis is commonly called the $n$th residual and is denoted $r_n = t_n - h(x_n, \mathbf{w})$. It’s the difference between the output of our polynomial for input variable $x_n$ and the corresponding target variable $t_n$. The difference can be both negative and positive depending on whether the output of our polynomial is lower or higher than the target.&lt;span class=&quot;marginnote&quot;&gt;Note that since we’re squaring all the differences, the value of the objective function $E$ cannot be lower than 0 - and if it’s exactly 0, then our model is making no mistakes; it is predicting the exact value of the target every time.&lt;/span&gt; We therefore square these differences and add them all up in order to get a value that tells us how our polynomial is performing.&lt;/p&gt;

&lt;!---
INSERT PICTURE/ANIMATION OF RESIDUALS
--&gt;

&lt;p&gt;So far, so good! Since the objective function tells us how well we’re doing, and the lower it is, the better we’re doing, we will try and find the minimum of the objective function. That is, we want to find the values for our parameters $\mathbf{w}$ that give us the lowest value for $E$. The process of determining the values for our parameters is called the &lt;strong&gt;training&lt;/strong&gt; or &lt;strong&gt;learning&lt;/strong&gt; process.&lt;/p&gt;

&lt;p&gt;Recall from the &lt;a href=&quot;http://localhost:4000/bslialo-notes-9b&quot;&gt;notes about extrema&lt;/a&gt; that to find the minimum of a function, we take the derivative, set it equal to 0, and solve for our parameters $\mathbf{w}$. Since we have a lot of parameters, we’ll take the partial derivative of $E$ with respect to the $i$th parameter $w_i$, set it equal to 0, and solve for it. This will give us a linear system of $M+1$ equations with $M+1$ unknowns (our parameters $\mathbf{w}$). We’ll go over the derivation of the solution to this problem in the next post, but for now we’ll just have it given. We get&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mathbf{w}} = \left( \mathbf{X}^\intercal \mathbf{X} \right)^{-1} \mathbf{X}^\intercal \textbf{\textsf{t}}, \quad \quad (3)&lt;/script&gt;

&lt;p&gt;where $\textbf{\textsf{t}}$ denotes all our target variables as a column vector $\textbf{\textsf{t}} = \left( t_1, t_2, \dots, t_N \right)^\intercal$, and $\mathbf{X}$ is called the &lt;strong&gt;design matrix&lt;/strong&gt; and is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{X} = \begin{pmatrix}
1 &amp; x_1 &amp; x_1^2 &amp; \cdots &amp; x_1^M \\
1 &amp; x_2 &amp; x_2^2 &amp; \cdots &amp; x_2^M \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_N &amp; x_N^2 &amp; \cdots &amp; x_N^M \\
\end{pmatrix}. \quad \quad (4) %]]&gt;&lt;/script&gt;

&lt;p&gt;To sum up: we’re given $N$ pairs of input and target variables $\left\{ (x_1, t_1), \dots, (x_N, t_N) \right\}$, and we want to fit a polynomial to the data of the form $(1)$ such that the value of $h(x_i, \mathbf{w})$ is as close to $t_i$ as possible. We do this by finding values for the parameters that minimize the objective function defined in $(2)$, and the solution is given by $(3)$.&lt;/p&gt;

&lt;h4 id=&quot;python-implementation-of-polynomial-regression&quot;&gt;Python implementation of polynomial regression&lt;/h4&gt;
&lt;p&gt;Let’s try and implement our model! We’ll start with the dataset shown underneath, where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt; is our input variables and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t&lt;/code&gt; is our target variables.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;To begin with we can define the order of our polynomial, find the number of data points, and then set up our design matrix.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;M&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If we look at the definition of the design matrix in $(4)$, we can fill out the columns of our design matrix with a for-loop.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now we can find the parameters with the solution in $(3)$.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@&lt;/code&gt; performs matrix multiplication.&lt;/span&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Using NumPy’s &lt;a href=&quot;https://docs.scipy.org/doc/numpy/reference/generated/numpy.poly1d.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;poly1d&lt;/code&gt; function&lt;/a&gt; we can generate outputs for our polynomial.
&lt;!--
MAYBE SHOW SMALL TEST OF poly1d FUNCTION
--&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt;We flip the weights to accommodate the input of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;poly1d&lt;/code&gt; function.&lt;/span&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;poly1d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linspace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now we can plot our estimated polynomial with our data points. I’ve also plotted the true function that the points were generated from.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/extra/bsmalea-notes-1a/poly_reg.svg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Machine learning studies &lt;strong&gt;how to make computers learn on their own&lt;/strong&gt; with the goal of &lt;strong&gt;predicting the future&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Supervised learning&lt;/strong&gt; refers to machine learning tasks, where we are given &lt;strong&gt;labeled data&lt;/strong&gt;, and we want to predict those labels.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Unsupervised learning&lt;/strong&gt;, as it suggests, refers to tasks, where we are &lt;em&gt;not&lt;/em&gt; provided with labels for our data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Features&lt;/strong&gt; refer to the &lt;strong&gt;attributes&lt;/strong&gt; (usually columns) of our data e.g. height, weight, shoe size, etc., if our observations are humans.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Classification and regression are supervised&lt;/strong&gt; tasks, &lt;strong&gt;clustering and density estimation are unsupervised&lt;/strong&gt; tasks.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Parameters&lt;/strong&gt; refer to the values, &lt;strong&gt;we want to estimate&lt;/strong&gt; in a machine learning model.&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;process of estimating the values of the parameters&lt;/strong&gt; is called the &lt;strong&gt;training or learning&lt;/strong&gt; process.&lt;/li&gt;
  &lt;li&gt;An &lt;strong&gt;objective function&lt;/strong&gt; is a &lt;strong&gt;measure of the performance&lt;/strong&gt; of our model.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 23 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://localhost:4000//bsmalea-notes-1a/</link>
        <guid isPermaLink="true">http://localhost:4000//bsmalea-notes-1a/</guid>
      </item>
    
      <item>
        <title>A look at Machine Learning</title>
        <description>&lt;p&gt;This hopefully won’t take as long as my notes on &lt;a href=&quot;http://localhost:4000/linear_algebra_and_optimisation&quot;&gt;Linear Algebra and Optimization&lt;/a&gt;, but I will start writing up my notes for the course “&lt;a href=&quot;https://mit.itu.dk/ucs/cb_www/course.sml?course_id=2013612&amp;amp;mode=search&amp;amp;lang=en&amp;amp;print_friendly_p=t&amp;amp;goto=1542111182.000&quot;&gt;Machine Learning&lt;/a&gt;” at the &lt;a href=&quot;https://www.itu.dk/&quot;&gt;IT University of Copenhangen&lt;/a&gt;. My goal is as always to explain the intuition behind the introduced concepts, making it easier to understand and engage with. I believe this can help the few who struggle to understand the concepts, as well as found the concepts for those who know how to employ them but lack the why.&lt;/p&gt;

&lt;h3 id=&quot;overview&quot;&gt;Overview&lt;/h3&gt;
&lt;p&gt;If you click the link above, you’ll see that there are 4 intended learning outcomes for the course; the student should be able to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Discuss&lt;/strong&gt;, clearly &lt;strong&gt;explain&lt;/strong&gt;, and &lt;strong&gt;reflect&lt;/strong&gt; upon central machine learning concepts and algorithms.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Choose among&lt;/strong&gt; and &lt;strong&gt;make use&lt;/strong&gt; of the most important machine learning approaches in order to apply (match) them to practical problems.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Implement&lt;/strong&gt; abstractly specified machine learning methods in an imperative programming language.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Combine&lt;/strong&gt; and &lt;strong&gt;modify&lt;/strong&gt; machine learning methods to &lt;strong&gt;analyse&lt;/strong&gt; practical dataset and &lt;strong&gt;convey&lt;/strong&gt; the results.&lt;/li&gt;
&lt;/ul&gt;

&lt;!--more--&gt;

&lt;p&gt;I believe, these can be summarized in a one-liner: the student should &lt;strong&gt;comprehend&lt;/strong&gt; various machine learning methods, and be able to &lt;strong&gt;interpret&lt;/strong&gt; their results to &lt;strong&gt;examine&lt;/strong&gt; data. To elaborate on this a bit, the course is about understanding mathematical formulations of machine learning methods and implementing them, which is why I’ll try to incorporate code snippets into these notes. It’s important to stress that the course is not a course in any specific machine learning library like TensorFlow, it’s not about deep learning, and it’s not about deriving models mathematically.&lt;/p&gt;

&lt;p&gt;Both statistics and linear algebra are prerequisites for machine learning. Not every little theorem is important, but the overall understanding and ability to use tools from statistics and linear algebra will be immensely helpful. Luckily, the course starts out with an introductory week, where the basics of machine learning are explained and a bit of probability theory is revised.&lt;/p&gt;

&lt;p&gt;Underneath is an overview of the notes, I’ll be writing. They more or less follow the structure of the course.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Introduction to machine learning:&lt;br /&gt;
 (a) &lt;a href=&quot;http://localhost:4000/bsmalea-notes-1a&quot;&gt;What is machine learning?&lt;/a&gt;&lt;br /&gt;
 (b) &lt;a href=&quot;http://localhost:4000/bsmalea-notes-1b&quot;&gt;Model selection and validation, the “no free lunch” theorem, and the curse of dimensionality&lt;/a&gt;&lt;br /&gt;
 (c) Frequentism and Bayesianism&lt;br /&gt;
 (d) Decision and information theory&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://localhost:4000/bsmalea-notes-2&quot;&gt;Regression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Classification:&lt;br /&gt;
 (a) Overview of classifiers&lt;br /&gt;
 (b) Distribution-free models&lt;br /&gt;
 (c) Discriminative models (logistic regression)&lt;br /&gt;
 (d) Generative models&lt;/li&gt;
  &lt;li&gt;Neural networks (feed-forward, backprop)&lt;/li&gt;
  &lt;li&gt;SVM (SMO), kernel methods&lt;/li&gt;
  &lt;li&gt;Graphical models&lt;/li&gt;
  &lt;li&gt;Mixture models and expectation-maximization (EM)&lt;/li&gt;
  &lt;li&gt;Sequential data (HMM)&lt;/li&gt;
  &lt;li&gt;Dimensionality reduction (LDA), continuous latent variables (PCA)&lt;/li&gt;
  &lt;li&gt;Combining models (ensemble methods, AdaBoost)&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Mon, 23 Sep 2019 00:00:00 -0400</pubDate>
        <link>http://localhost:4000//a-look-at-machine-learning/</link>
        <guid isPermaLink="true">http://localhost:4000//a-look-at-machine-learning/</guid>
      </item>
    
      <item>
        <title>The bare minimum guide to LaTeX</title>
        <description>&lt;p&gt;A few people have been asking for a short guide to LaTeX, but since I don’t have that much time on my hands these days, I thought I’d start with a very short guide that I can expand if needed.&lt;/p&gt;

&lt;h3 id=&quot;why&quot;&gt;Why?&lt;/h3&gt;
&lt;p&gt;You can think of LaTeX as a programming language for documents. It’s well-suited for large projects and complicated document structures, as it keeps track of a bunch of things by itself (we won’t get into those things in this post though). The other huge plus for LaTeX is the ability to write complicated mathematical equations simple and beautiful.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h3 id=&quot;getting-started-with-overleaf&quot;&gt;Getting started with Overleaf&lt;/h3&gt;
&lt;p&gt;While it’s possible to install LaTeX and a LaTeX editor on your computer, I think using Overleaf (version 2) is the best choice for most; you won’t have to find a proper LaTeX installation, worry about a million LaTeX files, deal with packages, figure out how to collaborate with other people, or worry about having access to your documents on different computers.&lt;/p&gt;

&lt;p&gt;Firstly, go to &lt;a href=&quot;https://www.overleaf.com?r=094710d5&amp;amp;rm=d&amp;amp;rs=b&quot;&gt;this link&lt;/a&gt;&lt;span class=&quot;sidenote-number&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sidenote&quot;&gt;This is a referral link that gives me a few bonuses on Overleaf - it won’t have any affect on you, but if you prefer not to use this, then go to Google and search for Overleaf v2.&lt;/span&gt; and sign up for an Overleaf (version 2) account. Secondly, go back to Overleaf and log in. Thirdly, you should see a green button in the top left that says “New Project”. Click the button and choose “Blank Project”. It will ask you to name your project, which can be whatever you want (it can be changed later if needed), and then press “Create”.&lt;/p&gt;

&lt;h3 id=&quot;the-structure-of-a-latex-document&quot;&gt;The structure of a LaTeX document&lt;/h3&gt;
&lt;p&gt;You should see a document on the right side of your screen, a bit of code in the middle, and an overview of your project on the left only containing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main.tex&lt;/code&gt;. If you don’t see the document on the right, then there should be a tiny button in the top right (below “Chat”) that looks like $2$ arrows pointing at each other. Hover over the button and it should say “Split screen”. If you click the button, your document should appear.&lt;/p&gt;

&lt;p&gt;Let’s look at the code now. I’ve posted below, what your code should look like (except for a few things).&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;k&quot;&gt;\documentclass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;article&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\usepackage&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;[utf8]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;inputenc&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;\title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;The bare minimum guide to LaTeX&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\author&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;Cookieblues&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;April 2019&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;\begin{document}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;\maketitle&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;\section&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;Introduction&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;\end{document}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;At first, it can look a bit daunting, but let’s try and decompose the different parts. The first line specifies what kind of document you want to write. In this case it defaults to an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;article&lt;/code&gt;, which is probably what most documents should be. The second line imports the package &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inputenc&lt;/code&gt;. The next three lines of code specifies some metadata about the document; the title, author and date. All of this is called the &lt;strong&gt;preample&lt;/strong&gt;, because it comes before the actual document. This is where, you can make changes to the structure of your document, e.g. the size of the margins, the size of the font, define specific functions, change the layout, etc. However, we won’t dive into all these things in this guide. We’ll only change the date from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\date{April 2019}&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\date{\today}&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Next up is the actual document, which is signified by the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\begin{document}&lt;/code&gt;, followed by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\end{document}&lt;/code&gt; when the document ends. This is where you’ll write your actual paper. To understand how to do this, we’ll have to learn a bit of LaTeX.&lt;/p&gt;

&lt;h3 id=&quot;writing-latex&quot;&gt;Writing LaTeX&lt;/h3&gt;
&lt;p&gt;As can be seen from the code above, most commands in LaTeX follow one of two standards: either &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\begin{command}&lt;/code&gt; followed by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\end{command}&lt;/code&gt; or just &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\command&lt;/code&gt;. Usually, a backslash &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\&lt;/code&gt; signifies a command, and the brackets&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;}&lt;/code&gt; is where you put the arguments for the command. An example is in the preample of our document, where we use the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\author{}&lt;/code&gt; and feed it the argument &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Cookieblues&lt;/code&gt;. Another is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\maketitle&lt;/code&gt; command in our document. It doesn’t take any arguments, but it writes the title, author, and date that we defined in our preample. We also use the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\section{}&lt;/code&gt;, which naturally creates a section in our document with the argument as the header. If we want to make a subsection in that section, we just use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\subsection{}&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;You might’ve noticed, there’s a number next to the section. This is used for your table of contents. Let’s add a table of contents right after our title and before our introduction. We can do this with the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\tableofcontents&lt;/code&gt;. If we don’t want to have a section or subsection appear in the table of contents (and thereby not have a number next to it), we can put an asterisk at the end of the section command like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\subsection*{No number}&lt;/code&gt;. Let’s add this subsection to our document as well. Our code should now look like the one below.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt;You might have noticed the small comment after the subsection. If you want to write comments in your LaTeX code, you can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;%&lt;/code&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;k&quot;&gt;\usepackage&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;[utf8]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;inputenc&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;\title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;The bare minimum guide to LaTeX&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\author&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;Cookieblues&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;\today&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;\begin{document}&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;\maketitle&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;\tableofcontents&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;\section&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;Introduction&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\subsection*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;No number&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;% this is a comment that won't appear in the document&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;\end{document}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;text&quot;&gt;Text&lt;/h3&gt;
&lt;p&gt;If you want to write something in your document, you just type it out. No need for special commands, you just write what you want, where you want it. I’ll write “&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;This is the introduction&lt;/code&gt;” under the introduction section, and I’ll write “&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;This is the subsection under the introduction&lt;/code&gt;”. The commands for &lt;strong&gt;bold&lt;/strong&gt; and &lt;em&gt;italic&lt;/em&gt; text are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\textbf{bold}&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\textit{italic}&lt;/code&gt; respectively.&lt;/p&gt;

&lt;h3 id=&quot;equations&quot;&gt;Equations&lt;/h3&gt;
&lt;p&gt;This is where LaTeX shines! And why it’s used for the vast majority of natural scientific papers. If you want to write an equation, you can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\begin{equation}&lt;/code&gt; command, write your equation, and end it with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\end{equation}&lt;/code&gt;. I use LaTeX equations on this website as well, and here are a couple of examples of regular functions:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
x+y
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x+y&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;2&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sqrt{2}&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\sin&lt;/span&gt;(x)
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sin(x)&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\tan&lt;/span&gt;(x)
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tan(x)&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;\exp&lt;/span&gt;(&lt;span class=&quot;k&quot;&gt;\cos&lt;/span&gt;(2x))&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sqrt{\exp(\cos(2x))}&lt;/script&gt;

&lt;h4 id=&quot;super--and-subscripts&quot;&gt;Super- and subscripts&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
x&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;2
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x^2&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
a&lt;span class=&quot;p&quot;&gt;_{&lt;/span&gt;12&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_{12}&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
e&lt;span class=&quot;p&quot;&gt;^{&lt;/span&gt;a+b&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; = e&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;a e&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;b
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e^{a+b} = e^a e^b&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;2(x) + &lt;span class=&quot;k&quot;&gt;\cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;2(x) = 1
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sin^2(x) + \cos^2(x) = 1&lt;/script&gt;

&lt;h4 id=&quot;fractions&quot;&gt;Fractions&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\frac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;1&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;2&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; = 1/2
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{2} = 1/2&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\frac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;1&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;\sin&lt;/span&gt;(x)&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{\sin(x)}&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\frac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;x+&lt;span class=&quot;k&quot;&gt;\frac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;1&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;}}{&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;2-1&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{x+\frac{1}{x}}{x^2-1}&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
(f/g)' = &lt;span class=&quot;k&quot;&gt;\frac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;f'g-fg'&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;g&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;2&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(f/g)' = \frac{f'g-fg'}{g^2}&lt;/script&gt;

&lt;h4 id=&quot;integrals-sums-and-other-operators&quot;&gt;Integrals, sums, and other operators&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\int&lt;/span&gt; x&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;3 dx + &lt;span class=&quot;k&quot;&gt;\sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_{&lt;/span&gt;n=1&lt;span class=&quot;p&quot;&gt;}^{&lt;/span&gt;N&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; n
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int x^3 dx + \sum_{n=1}^{N} n&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;0&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;1 &lt;span class=&quot;k&quot;&gt;\frac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;1&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; dx + &lt;span class=&quot;k&quot;&gt;\sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_{&lt;/span&gt;a,b&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; a-b
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_0^1 \frac{1}{x} dx + \sum_{a,b} a-b&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;0&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;\pi&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;\sin&lt;/span&gt; (x) dx + &lt;span class=&quot;k&quot;&gt;\sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_{&lt;/span&gt;n=1&lt;span class=&quot;p&quot;&gt;}^&lt;/span&gt;N &lt;span class=&quot;k&quot;&gt;\frac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;1&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_0^\pi \sin (x) dx + \sum_{n=1}^N \frac{1}{n}&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\int&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;\frac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;^{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;\sin&lt;/span&gt; (x)&lt;span class=&quot;p&quot;&gt;}}{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;\sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;\cos&lt;/span&gt; (x)&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt; dx + &lt;span class=&quot;k&quot;&gt;\sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_{&lt;/span&gt;n=1&lt;span class=&quot;p&quot;&gt;}^&lt;/span&gt;N &lt;span class=&quot;k&quot;&gt;\frac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;n&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;n+1&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int \frac{x^{\sin (x)}}{\sqrt{\cos (x)}} dx + \sum_{n=1}^N \frac{n}{n+1}&lt;/script&gt;

&lt;h4 id=&quot;greek-letters&quot;&gt;Greek letters&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
2&lt;span class=&quot;k&quot;&gt;\pi&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;2\pi&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
|x-a| &amp;lt; &lt;span class=&quot;k&quot;&gt;\delta&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
|x-a| &lt; \delta %]]&gt;&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\varphi&lt;/span&gt; = &lt;span class=&quot;k&quot;&gt;\frac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;1+&lt;span class=&quot;k&quot;&gt;\sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;5&lt;span class=&quot;p&quot;&gt;}}{&lt;/span&gt;2&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\varphi = \frac{1+\sqrt{5}}{2}&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;0&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;\pi&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;\sin&lt;/span&gt; (x) dx
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_0^\pi \sin (x) dx&lt;/script&gt;

&lt;h4 id=&quot;brackets&quot;&gt;Brackets&lt;/h4&gt;
&lt;p&gt;Square brackets are just plainly written &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;]&lt;/code&gt;, curly brackets require a backslash beforehand &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\{&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\}&lt;/code&gt; as they are otherwise used for wrapping arguments in LaTeX. If the expression is large, you can write &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\left&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\right&lt;/code&gt; before the paranthesis to make LaTeX adjust the size of them for your expression. Here are some examples:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\{&lt;/span&gt;1,2,3,4&lt;span class=&quot;k&quot;&gt;\}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\{1,2,3,4\}&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
y - &lt;span class=&quot;k&quot;&gt;\left&lt;/span&gt;( &lt;span class=&quot;k&quot;&gt;\frac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;1&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;\right&lt;/span&gt;)&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;2 = 0
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y - \left( \frac{1}{x} \right)^2 = 0&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\left&lt;/span&gt;( &lt;span class=&quot;k&quot;&gt;\int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;0&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;t &lt;span class=&quot;k&quot;&gt;\log&lt;/span&gt; (y) dy &lt;span class=&quot;k&quot;&gt;\right&lt;/span&gt;)&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;t
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left( \int_0^t \log (y) dy \right)^t&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-latex&quot; data-lang=&quot;latex&quot;&gt;&lt;span class=&quot;nt&quot;&gt;\begin{equation}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;\int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;a&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;b &lt;span class=&quot;k&quot;&gt;\frac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;x&lt;span class=&quot;p&quot;&gt;}{&lt;/span&gt;b-a&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; dx = &lt;span class=&quot;k&quot;&gt;\left&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;[ \frac{x}{b-a} \right]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;^&lt;/span&gt;b&lt;span class=&quot;p&quot;&gt;_&lt;/span&gt;a
&lt;span class=&quot;nt&quot;&gt;\end{equation}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_a^b \frac{x}{b-a} dx = \left[ \frac{x}{b-a} \right]^b_a&lt;/script&gt;

&lt;h3 id=&quot;final-tips&quot;&gt;Final tips&lt;/h3&gt;
&lt;p&gt;You might’ve noticed that your document didn’t change even though you changed your LaTeX code. This is because LaTeX is just that: code. There’s a big green button that says “Recompile” in the top left of your document. If you click it, the new code you have written will be compiled, and you will see it in your document (or you get an error, if there’s a mistake in your code). To the right of the button is a small downward pointing arrow - if you click that, you can turn “Auto compile” on or off. This will get Overleaf to compile your code as you’re writing.&lt;/p&gt;

&lt;p&gt;When you get started writing mathematics in LaTeX, often times you’ll find yourself in need of symbols that you don’t know the name of and definitely not the LaTeX code for. In these cases, you could either Google your way to the answer, or use the brilliant site &lt;a href=&quot;http://detexify.kirelabs.org/classify.html&quot;&gt;Detexify&lt;/a&gt;. You simply use your mouse to draw the symbol that you want, and Detexify tries to identify the most probable symbols that you’re looking for - and not only does it detect the symbol, but it also provides you with the package it’s from in case it requires one.
&lt;span class=&quot;marginnote&quot;&gt;I swear, I use Detexify so often - it’s worth a bookmark!&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;exercises&quot;&gt;Exercises&lt;/h3&gt;
&lt;p&gt;To practice writing equations, try and see if you can write the equations underneath.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a^z + b^z = c^z&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sqrt{\frac{n}{n-1}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e^{ix} = r ( \cos \theta + i \sin \theta )&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int \frac{\sin (x)}{x^2+1} dx&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_a^b x^{10} \sum_{i=1}^n i^2 dx&lt;/script&gt;

&lt;p&gt;These next ones require something I haven’t shown in this post, so a little bit of Googling or outside-the-box thinking is required.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;1+2+3+\cdots+n = \frac{n(n+1)}{2}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lim_{x \to x_0} \frac{f(x)-f(x_0)}{x-x_0} = c&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{\zeta (s)} = \prod_{p \text{ prime}} \left( 1 - \frac{1}{p^s} \right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\det
\begin{pmatrix}
    a &amp; b \\
    c &amp; d
\end{pmatrix}
= ad-bc %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\max \left\{ 0, \left| \frac{1}{a}-b \right| \right\}&lt;/script&gt;
</description>
        <pubDate>Thu, 04 Apr 2019 00:00:00 -0400</pubDate>
        <link>http://localhost:4000//bare-minimum-latex/</link>
        <guid isPermaLink="true">http://localhost:4000//bare-minimum-latex/</guid>
      </item>
    
      <item>
        <title>Analysis of the biyearly evaluations at the IT University of Copenhagen, part 1: Data scraping</title>
        <description>&lt;p&gt;The &lt;a href=&quot;https://www.itu.dk/&quot;&gt;IT University of Copenhagen (ITU)&lt;/a&gt; conducts student evaluations each semester of the courses and the university itself. The evaluations are one of the ways that students can provide feedback to the lecturers, which can improve the learning environment, course material, exercises, etc. The evaluations are also packed with interesting data, and naturally, as a data science student, I cannot keep my hands off them.&lt;/p&gt;

&lt;p&gt;This analysis of the evaluations will be a series of posts, where I’ll dive into some of the interesting data that we can get out of the evaluations. On top of that, these first few posts will serve as notes for the first project of the &lt;a href=&quot;https://mit.itu.dk/ucs/cb_www/course.sml?course_id=2708256&amp;amp;mode=search&amp;amp;lang=en&amp;amp;print_friendly_p=t&amp;amp;goto=1547640286.000&quot;&gt;First Year Project&lt;/a&gt; course. This time we’ll focus on elementary &lt;a href=&quot;https://en.wikipedia.org/wiki/Data_scraping&quot;&gt;data scraping&lt;/a&gt;, which in its broadest sense is the act of collecting data.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h3 id=&quot;browser-automation-with-selenium&quot;&gt;Browser automation with Selenium&lt;/h3&gt;
&lt;p&gt;To scrape data, firstly, we’ll need to find the place where the data, we’re interested in, is located and free to use. Luckily, the ITU has made the evaluations publicly available on &lt;a href=&quot;https://en.itu.dk/about-itu/organisation/facts-and-figures/quality-and-educational-environment/course-evaluation&quot;&gt;their website&lt;/a&gt;.
&lt;span class=&quot;marginnote&quot;&gt;Also, I spoke to someone at ITU, and they said that we can do anything we want with the evaluations, as long as the data is publicly available.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Secondly, now we’ll have to find a way to actually collect the data. The link on ITU’s website leads to a webpage, where we can choose specific evaluation results for any evaluation period we desire. While the number of evaluation periods is small enough for us to manually collect the results, there are several benefits to not collecting the results this way. First of all, by avoiding performing a task the naive way we might learn something new. Second of all, if our method is broad enough, it might generalize to other tasks and save us time in the long run. Third of all, if we were to manually collect the results, the naive way would be to download the HTML pages, and afterwards clean the data for our purpose - however, we can clean the data as we collect it, if we don’t do it the naive way, which will save us some time too&lt;/p&gt;

&lt;p&gt;Now, there are many ways to scrape data from websites, and depending on the specific website there might be a plethora of tools better suited for your purpose (public APIs, specific modules, etc.), especially if you want to scrape data from more popular sites. In this brief introduction to data scraping, we’ll utilize &lt;a href=&quot;https://github.com/SeleniumHQ/Selenium&quot;&gt;Selenium&lt;/a&gt;, which is definitely not the most efficient library for browser automation, but we don’t have a lot of data to scrape, and we want something that is user-friendly.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;selenium&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;webdriver&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;webdriver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Firefox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;https://mit.itu.dk/ucs/evaluation/public.sml?lang=english&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We’re going to use the WebDriver API from Selenium, which makes browser automation almost as intuitive as normal web browsing. By “getting” the URL, a new Firefox window will open, which is administered by the driver. We can now manoeuvre around on the webpage by using the driver’s functions and with a little bit of knowledge of &lt;a href=&quot;https://en.wikipedia.org/wiki/XPath&quot;&gt;XPath&lt;/a&gt;. By right-clicking on the webpage and looking at the page source, we can identify the radio buttons that dictate the results we get by their value. For now, we’ll look at the evaluations of ITU in general, which is the radio button of value 0. We find the element through the driver, and click on it.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_element_by_xpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;//input[@type='RADIO'][@value=0]&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;click&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Then we want to select the evaluation period. We can use one of the WebDriver’s support classes for this. With a bit of foresight, we can tell that we’re going to do this a lot, so let’s write a function that can do this for us. If we inspect the drop-down list, we can see that the different choices are called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2018october&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2018march&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2017october&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2017march&lt;/code&gt;, etc. (except for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2002april&lt;/code&gt; which is the odd one out). We can use this as input in the function (as well as the driver). The second thing we can use is the submit method of a form in Selenium. By inspecting the site a bit more, we find that the evaluation results and periods all lie in a form. After choosing our desired evaluation results and period, we can submit the form and get to the next page.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;selenium.webdriver.support.ui&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Select&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;enter_eval_period&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;period&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;evaluation_form&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_element_by_xpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;//form[1]&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;evaluation_period&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;evaluation_form&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_element_by_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dir&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;evaluation_period&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select_by_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;period&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;evaluation_form&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;submit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;enter_eval_period&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;2018october&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The next page lets us choose, which students (part- or full-time) we want the evaluations for. While I don’t want the analysis to be overly thorough, I will scrape the evaluations from part-time students and full-time students individually - even though there are way fewer part-time students. Since both boxes are checked, we’ll just make a function that unchecks the full-time students.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;pick_parttime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;student_form&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_element_by_xpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;//form[1]&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;student_form&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_element_by_xpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;//input[@type='CHECKBOX'][@value='orduni']&quot;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;click&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;student_form&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;submit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pick_part_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;scraping-data-with-beautifulsoup-and-pandas&quot;&gt;Scraping data with BeautifulSoup and Pandas&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.crummy.com/software/BeautifulSoup/&quot;&gt;BeautifulSoup&lt;/a&gt; is a wonderful library for parsing HTML documents, and it works naturally together with &lt;a href=&quot;https://pandas.pydata.org/&quot;&gt;Pandas&lt;/a&gt;. We’re currently in the part-time students’ evaluations for October, 2018. We have some numerical data at the top of the page in a table, and the textual data is underneath in lists. To get the table data, we can use BeautifulSoup to parse the page source, find the table we need, and then use Pandas to put the table in a DataFrame.
&lt;span class=&quot;marginnote&quot;&gt;Pandas is &lt;em&gt;the&lt;/em&gt; data analysis tool in Python, if you didn’t know already.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt;We parse the current page of the driver with BeautifulSoup, find all tables, and take the third one (the one we need). Then Pandas’ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;read_html&lt;/code&gt; function returns a list of dataframes for each of the tables, but since we only gave it the one table we needed, we just need the zeroth index. This gives us the numerical data in a DataFrame.&lt;/span&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bs4&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BeautifulSoup&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_numerical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;soup_page&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BeautifulSoup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;page_source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;soup_page&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;table&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_html&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Unnamed: 0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;statement&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;numerical_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_numerical&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The next thing we need is the textual data. This is a bit more complicated, since the textual data is wrapped in a lot of lists. By inspecting the page source, we can see that all the textual evaluations are wrapped in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ul&lt;/code&gt; tags. By further inspection, every 4th &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ul&lt;/code&gt; tag are “Good things about the IT University”, and the ones following are “Things that could be improved”. We could grab all the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ul&lt;/code&gt; tags and use some regular expressions to get what we want, &lt;a href=&quot;https://stackoverflow.com/a/1732454&quot;&gt;but you should not attempt to parse HTML with regex&lt;/a&gt;. In each &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ul&lt;/code&gt; tag we have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;li&lt;/code&gt; tags, where only a few contain text that we need. The function below gets us exactly what we want.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_textual_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;soup_page&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BeautifulSoup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;page_source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;good&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ul&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;soup_page&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ul&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;bad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;evaluation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluation&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ul&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;li&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;good&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;evaluation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluation&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ul&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;li&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Series&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;good&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Series&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;good&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bad&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;textual_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_textual_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;You might have noticed that I called this function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_textual_1&lt;/code&gt;, and that’s because a few of the evaluations follow a slightly different template. For the other template we can use the following function, which follows the same idea.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_textual_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;soup_page&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BeautifulSoup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;page_source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;good&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ol&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;soup_page&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ol&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;bad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;evaluation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluation&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;li&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;good&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;evaluation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluation&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;li&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Series&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;good&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Series&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;good&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bad&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;textual_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_textual_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If we use the wrong function for an evaluation template, we’ll return an empty DataFrame. We can therefore make one function that takes care of both scenarios.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_textual&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_textual_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_textual_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_textual_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We can now go back, enter the full-time students’ evaluations, scrape their data, and go all the way back to choosing a different period. A simple back method exists for the driver.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Coming back from the part-time students, we have to uncheck the part-time students and check the full-time students.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;pick_fulltime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;students_form&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;driver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_element_by_xpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;//form[1]&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;students_form&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_element_by_xpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;//input[@type='CHECKBOX'][@value='openuni']&quot;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;click&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;students_form&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_element_by_xpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;//input[@type='CHECKBOX'][@value='orduni']&quot;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;click&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;students_form&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;submit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;That’s it! We can now go through all the different evaluation periods, scrape the data, and save the DataFrames.&lt;/p&gt;

</description>
        <pubDate>Thu, 17 Jan 2019 00:00:00 -0500</pubDate>
        <link>http://localhost:4000//itu-eval-part-01/</link>
        <guid isPermaLink="true">http://localhost:4000//itu-eval-part-01/</guid>
      </item>
    
      <item>
        <title>BSLIALO, notes 9c: Integration and the fundamental theorem of calculus</title>
        <description>&lt;p&gt;In &lt;a href=&quot;http://localhost:4000/bslialo-notes-9a&quot;&gt;notes 9a&lt;/a&gt; we introduced the derivative by looking at a $100$ meter sprint race. We looked at the so-called distance function $s(t)$, which told us how far we’d run in the race at time $t$, and asked if we could figure out a function for velocity $v(t)$. We found that the derivative of the distance function $s(t)$ was the velocity function $v(t)$, which we wrote up&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{d}{dt}s(t) = s'(t) = v(t).&lt;/script&gt;

&lt;p&gt;This time we will look at the velocity function and try to figure out the distance function, so we want to do the opposite of last time, where we found the derivative; we want to find the &lt;strong&gt;antiderivative&lt;/strong&gt;.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h3 id=&quot;integration&quot;&gt;Integration&lt;/h3&gt;
&lt;p&gt;Let’s take the example from last time but focus only on the velocity. If we want to find the antiderivative, we could ask ourselves: what function when taking the derivative gives us the velocity function? In that case, we have essentially solved our problem of finding the antiderivative, but we’re going to take a different approach. Let’s say you run a constant $10$ meters per second, so you finish the $100$ meter sprint in $10$ seconds. If we plot the velocity function with time on the x-axis, and velocity $\left( \frac{\text{distance}}{\text{time}} \right)$ on the y-axis (like below), we can think of area on the plot as distance, since&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{velocity} \times \text{time} = \frac{\text{distance}}{\text{time}} \times \text{time} = \text{distance}.&lt;/script&gt;

&lt;video width=&quot;500&quot; height=&quot;310&quot; loop=&quot;&quot; muted=&quot;&quot; autoplay=&quot;&quot;&gt;
    &lt;source src=&quot;http://localhost:4000/extra/bslialo-notes-9c/fig_02.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;I know it sounds weird to have distance be area, but try and stick with it. The plot above illustrates, how the area equals the distance traveled. Take some time to study the plot above, and let this idea sink in. Now, it’s easy to find the area, when the velocity is constant, since we’re just dealing with a rectangle - so let’s try and step it up a notch!&lt;/p&gt;

&lt;p&gt;Let’s say, the velocity function follows the parabola $v(t)=t(10-t)=10t-t^2$. A quick &lt;a href=&quot;http://localhost:4000/bslialo-notes-9b&quot;&gt;extrema&lt;/a&gt; analysis tells us, this function increases from $v(t) = 0$ at $t=0$ up to a maximum $v(t) = 25$ at $t=5$, and then decreases again to $v(t)=0$ at $t=10$. To find the area between this velocity function and the x-axis, we could use the rectangle as inspiration and try to fill rectangles under the curve. Let’s chop the function up in $6$ rectangles, making their widths equal $\frac{10}{6}$ - and if we choose the rectangles’ upper left corners to touch the function, then it would look something like the plot below.&lt;/p&gt;

&lt;video width=&quot;500&quot; height=&quot;310&quot; loop=&quot;&quot; muted=&quot;&quot; autoplay=&quot;&quot;&gt;
    &lt;source src=&quot;http://localhost:4000/extra/bslialo-notes-9c/fig_03.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;We end up with $6$ rectangles as wanted, where each rectangle’s height is given by the point where the upper left corner touches the velocity function. The height of the first rectangle is $v(0) = 0\cdot (10-0) = 0$, the height of the second rectangle is $v \left( \frac{10}{6} \right) = \frac{10}{6} \cdot \left( 10 - \frac{10}{6} \right) \approx 14$, the height of the third rectangle is $v \left( \frac{20}{6} \right) = \frac{20}{6} \cdot \left( 10 - \frac{20}{6} \right) \approx 22$, etc. The widths of all the rectangles are obviously $\frac{10}{6}$, as we defined, so we can write the area as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\text{Area}
&amp;= \frac{10}{6} \cdot v(0) + \frac{10}{6} \cdot v \left( \frac{10}{6} \right) + \frac{10}{6} \cdot v \left( \frac{20}{6} \right) + \cdots + \frac{10}{6} \cdot  v \left( \frac{50}{6} \right).
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;If we call the points where the rectangles touch the velocity function $t_0, t_1, \dots, t_5$, we can write this as the sum&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Area} = \sum_{n=0}^{5} v \left( t_n \right) \cdot \frac{10}{6}.&lt;/script&gt;

&lt;p&gt;We can generalize this even further, if we let $N$ be the number of rectangles, and $a,b$ be the endpoints of the interval. So in our case $N=6$, and $a=0,b=10$. We could write this as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Area} = \sum_{n=0}^{N-1} v \left( t_n \right) \cdot \frac{b-a}{N}. \quad \quad (1)&lt;/script&gt;

&lt;p&gt;Obviously, we are not going to get a good approximation of the area under the velocity function with only six rectangles, but by increasing the number of rectangles our approximation will get closer and closer to the actual area. To do that, we want the width of the rectangles to get &lt;strong&gt;infinitesimally&lt;/strong&gt; small, which means, they’re as close to $0$ as possible. We can interpret this another way; if we want the width of the rectangles $\left( \frac{b-a}{N} \right)$ to get infinitesimally small, we can take the &lt;a href=&quot;http://localhost:4000/bslialo-notes-9a&quot;&gt;limit&lt;/a&gt; of $(1)$ as $N$ approaches infinity&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Area} = \lim_{N \to \infty} \sum_{n=0}^{N-1} v \left( t_n \right) \cdot \frac{b-a}{N}. \quad \quad (2)&lt;/script&gt;

&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt;Illustration of how the area under the curve is refined as the number of rectangles increase.&lt;/span&gt;&lt;/p&gt;
&lt;figure&gt;
    &lt;video width=&quot;500&quot; height=&quot;310&quot; loop=&quot;&quot; muted=&quot;&quot; autoplay=&quot;&quot;&gt;
        &lt;source src=&quot;http://localhost:4000/extra/bslialo-notes-9c/fig_04.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;This is the intuition behind integration, and what we wrote in $(2)$ is the general idea of an integral&lt;span class=&quot;sidenote-number&quot;&gt;&lt;/span&gt;.
&lt;span class=&quot;sidenote&quot;&gt;It’s a loose interpretation of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Riemann_integral#Riemann_integral&quot;&gt;Riemann integral&lt;/a&gt;.&lt;/span&gt;
Usually, we rewrite $(2)$ as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lim_{N \to \infty} \sum_{n=0}^{N-1} v \left( t_n \right) \Delta t, \quad \quad (3)&lt;/script&gt;

&lt;p&gt;where $\Delta t = \frac{b-a}{N}$ to signify the width, and $t_n = a + n \cdot \Delta t$. This is called a &lt;strong&gt;definite integral&lt;/strong&gt;, which is an integral with a lower and upper limit, $a$ and $b$ respectively. To shorten this even further, we use the integral sign $\int$, and write $(3)$ as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_a^b v(t) dt. \quad \quad (4)&lt;/script&gt;

&lt;p&gt;The integral sign represents the sum of the infinitesimally narrow rectangles between $a$ and $b$ under the function $v(t)$. We read $(4)$ as: “the integral from $a$ to $b$ of $v(t)$”.&lt;/p&gt;

&lt;h3 id=&quot;the-fundamental-theorem-of-calculus&quot;&gt;The fundamental theorem of calculus&lt;/h3&gt;
&lt;p&gt;Let’s take a moment to go through, what we’ve just done. Our original question was, whether or not we could find a distance function purely by looking at the velocity function. We reasoned that since velocity $\times$ time $=$ distance, then the area underneath the velocity function from start to finish, would be the distance traveled from start to finish. To calculate the area underneath the velocity function, we chopped it up into a bunch of infinitesimally narrow rectangles, and then found the area of the rectangles with the width and height of each of them. From this, we can say&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s(T) = \int_0^T v(t) dt, \quad \quad (5)&lt;/script&gt;

&lt;p&gt;i.e., the distance traveled from $t=0$ to $t=T$ (or $10$ if we want ten seconds) is the same as the integral from $t=0$ to $t=T$ of the velocity function. In &lt;a href=&quot;http://localhost:4000/bslialo-notes-9a&quot;&gt;notes 9a&lt;/a&gt; we talked about the derivative as the instantaneous rate of change of a function; if we consider $s(T)$ (the area under the velocity function) as a function on its own, we can deduce its derivative. If we denote the instantaneous change in time as $dT$, then the change in area can be approximated by a rectangle as before, where the height is the velocity function $v(T)$. In other words, if the time changes by a tiny amount $dT$, then the area under the velocity function changes by the rectangle $v(t)dT$, which gives us&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
ds(T) &amp;= v(T)dT \\
\frac{d}{dT}s(T) &amp;= v(T).
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;This is the first part of the fundamental theorem of calculus&lt;span class=&quot;sidenote-number&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sidenote&quot;&gt;It’s not the rigorous &lt;a href=&quot;https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus&quot;&gt;theorem&lt;/a&gt;.&lt;/span&gt;; let $f$ be some function, and $F$ the function of the area under $f$ (the integral of $f$), i.e.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(x) = \int_a^x f(t) dt,&lt;/script&gt;

&lt;p&gt;then the derivative of $F$ is equal to the original function $f$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F'(x) = f(x). \quad \quad (6)&lt;/script&gt;

&lt;p&gt;The next question to tackle is: how do we compute this?&lt;/p&gt;

&lt;p&gt;Well, $(6)$ gives us a hint that we actually mentioned in the beginning. To find the integral we compute the &lt;strong&gt;indefinite integral&lt;/strong&gt; or the &lt;strong&gt;antiderivative&lt;/strong&gt;, which is the integral without a lower or upper limit. We ask ourselves: what function when taking the derivative gives us $v(t)=10t-t^2$? We know that the derivative of $t^3$ is $3t^2$, so we can multiply this by $-1/3$ to get the second term - and the derivative of $t^2$ is $2t$, so multiplying this by $5$ gives us the first term. At the end we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\int v(t) dt &amp;= \int 10t-t^2 dt \\
&amp;= 5t^2 - \frac{1}{3} t^3 + c.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The $c$ is the potential constant that we lose after taking the derivative.
&lt;span class=&quot;marginnote&quot;&gt;The derivative of a constant is just $0$, therefore when we integrate (do the opposite of differentiate), we add the constant back.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The second and final part of the fundamental theorem of calculus follows from the first part. Given $f$ and its antiderivative $F$, we can say&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_a^b f(x) dx = F(b)-F(a).&lt;/script&gt;

&lt;h4 id=&quot;example&quot;&gt;Example:&lt;/h4&gt;
&lt;p&gt;Let’s take the definite integral&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\int_{1}^e \frac{1}{x} dx,
\end{aligned}&lt;/script&gt;

&lt;p&gt;where $e$ is &lt;a href=&quot;https://en.wikipedia.org/wiki/E_(mathematical_constant)&quot;&gt;Euler’s number&lt;/a&gt;. We start by finding the antiderivative of $\frac{1}{x}$, which from &lt;a href=&quot;http://localhost:4000/bslialo-notes-9b&quot;&gt;notes 9b&lt;/a&gt; we know is $\ln x$, and then we can compute the integral&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
\int_{1}^e \frac{1}{x} dx = \ln(e) - \ln(1) = 1 - 0 = 1.
\end{aligned}&lt;/script&gt;

</description>
        <pubDate>Thu, 15 Nov 2018 00:00:00 -0500</pubDate>
        <link>http://localhost:4000//bslialo-notes-9c/</link>
        <guid isPermaLink="true">http://localhost:4000//bslialo-notes-9c/</guid>
      </item>
    
      <item>
        <title>BSLIALO, notes 9b: The chain rule, derivatives of logarithmic functions, extrema, and optimisation</title>
        <description>&lt;p&gt;Differentiation is commonly taught through rote learning, which I’m not a fan of. Therefore I’ll allow myself to skip the basic rules of differentiation, and what the derivative of every little function is. Instead, I want to focus on the useful and interesting things, namely, the chain rule and logarithmic derivatives, both of which are important in machine learning. At the end, we’ll talk about optimisation, where we’ll try and combine, what we’ve learned.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-chain-rule&quot;&gt;The chain rule&lt;/h3&gt;
&lt;p&gt;There are many rules of differentiation, e.g. let $u,v$ be functions, then the sum rule tells us how to take the derivative of a sum of functions&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{d}{dx} (u + v) = \frac{du}{dx} + \frac{dv}{dx},&lt;/script&gt;

&lt;p&gt;the product rule tells us how to take the derivative of a product of functions&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{d}{dx} (u \cdot v) = \frac{du}{dx} \cdot v + u \cdot \frac{dv}{dx},&lt;/script&gt;

&lt;p&gt;and the chain rule tells us how to take the derivative of a &lt;strong&gt;composition&lt;/strong&gt; of functions. A function composition can be thought of as putting one function inside another. Say we have two functions $f(x) = \sin (x)$ and $g(x) = x^2$. We can make two compositions from these two functions:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt;The $\circ$ symbol represents a function composition.&lt;/span&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
(f \circ g) (x) &amp;= f(g(x)) = \sin \left( x^2 \right), \text{ and} \\
(g \circ f) (x) &amp;= g(f(x)) = \sin^2(x).
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Let’s choose $(f \circ g) (x) = \sin \left( x^2 \right)$ and try to reason our way to its derivative. From &lt;a href=&quot;http://localhost:4000/bslialo-notes-9a&quot;&gt;notes 9a&lt;/a&gt; we know, the derivative of a function is the oxymoron &lt;strong&gt;instantaneous rate of change&lt;/strong&gt;. If we call this change $dx$, then we can write the derivative as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{du}{dx} = \lim_{ dx \to 0} \frac{u(x+ dx)-u(x)}{ dx}. \quad \quad (1)&lt;/script&gt;

&lt;p&gt;We can also phrase this as: a slight change in the function’s output divided by a slight change in its input. In our example of function composition, $(f \circ g) (x) = \sin \left( x^2 \right)$, we have to think about this in two steps.&lt;/p&gt;

&lt;p&gt;Firstly, we have to consider, what happens to $g(x)$, when a slight change in $x$ occurs. Since $g(x) = x^2$, a slight change $dx$ will equal $g(x + dx) - g(x)$ or rather&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
g(x + dx) - g(x)
&amp;= (x+dx)^2 - x^2 \\
&amp;= x^2 + 2xdx + (dx)^2 - x^2 \\
&amp;= 2xdx + ( dx )^2.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;If we divide both sides by the slight change $dx$ and compare it to equation $(1)$, we see, we’ve calculated the derivative of $g$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{dg}{dx} = \frac{g(x + dx) - g(x)}{dx} = 2x + dx, \quad \quad (2)&lt;/script&gt;

&lt;p&gt;which is $2x$, plus a slight change that will be negligible, when $dx$ approaches $0$.&lt;/p&gt;

&lt;p&gt;Secondly, we have to consider, what happens to $f(g(x))$, when a slight change in $g(x)$ occurs. Using the same reasoning as before, and knowing that the derivative of $\sin(x)$ is $\cos(x)$, we can say&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{df}{dg} = \cos(g) \quad \quad (3)&lt;/script&gt;

&lt;p&gt;as $dg \to 0$, i.e. the derivative of $\sin(g)$ with respect to $g$ is $\cos(g)$. Multiplying $(2)$ and $(3)$ yields&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{df}{dg} \cdot \frac{dg}{dx} &amp;= \cos(g) 2x \\
\frac{df}{dx} &amp;= \cos( x^2 ) 2x,
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;which is the derivative of $(f \circ g)(x) = \sin \left( x^2 \right)$ with respect to $x$. This result is the derivative of $f$ with respect to $g$ multiplied by the derivative of $g$ with respect to $x$, or rather the derivative of the “outer” function $(f)$ multiplied by the derivative of the “inner” function $(g)$. That’s the chain rule.&lt;/p&gt;

&lt;p&gt;More formally, if $g$ is a function differentiable at $a$, and $f$ is a function differentiable at $g(a)$, then the composite function $h = f \circ g$ is differentiable at $a$, and the derivative is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h'(a) = (f \circ g)'(a) = f'(g(a)) g'(a). \quad \quad (4)&lt;/script&gt;

&lt;h4 id=&quot;example&quot;&gt;Example:&lt;/h4&gt;
&lt;p&gt;Let’s try and find the derivative of $f(x) = \sqrt[3]{7-3x}$. We start by rewriting the root as a power $f(x) = (7-3x)^{1/3}$. Then we call the inner function $g(x)=7-3x$, and the outer function $h(g) = g^{1/3}$.&lt;/p&gt;

&lt;p&gt;The derivative of $g$ with respect to $x$ is $-3$, and the derivative of $h(g)$ with respect to $g$ is $\frac{1}{3} g^{-2/3}$. According to the chain rule $(4)$, the product of these gives us the derivative&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{df}{dx}
&amp;= -3 \cdot \frac{1}{3} (7-3x)^{-\frac{2}{3}} \\
&amp;= -\frac{1}{(7-3x)^{\frac{2}{3}}}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;derivatives-of-logarithmic-functions&quot;&gt;Derivatives of logarithmic functions&lt;/h3&gt;
&lt;p&gt;It’s tempting to dive into the relationship between derivatives, exponential, and logarithmic functions. Instead we will briefly talk about logarithms, some of their properties, and then look at the usefulness of logarithms, when we get to optimisation, which is also an appetizer for &lt;a href=&quot;https://en.wikipedia.org/wiki/Maximum_likelihood_estimation&quot;&gt;maximum likelihood estimation&lt;/a&gt; in machine learning. However, before we get into the derivative of logarithms, let’s refresh logarithms themselves, shall we?&lt;/p&gt;

&lt;p&gt;Firstly, a logarithm is the inverse function to an exponential function, i.e. $x$ is the base-$b$ logarithm of $y$ if and only if $b^x = y$. This is written&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log_b y = x \iff b^x = y.&lt;/script&gt;

&lt;p&gt;Logarithms are used to solve questions like: what exponent should 3 be raised to to equal 81?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
3^x &amp;= 81 \\
x &amp;= \log_3 81 \\
x &amp;= 4.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The &lt;strong&gt;common logarithm&lt;/strong&gt; is the logarithm to base $10$ denoted $\log(x)$. The &lt;strong&gt;natural logarithm&lt;/strong&gt;, which is the one we’ll be using the most, has &lt;a href=&quot;https://en.wikipedia.org/wiki/E_(mathematical_constant)&quot;&gt;Euler’s number&lt;/a&gt; $e$ as its base and is denoted $\ln(x)$.&lt;/p&gt;

&lt;p&gt;Logarithms are useful for their special properties - the most common being&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\log_b x + \log_b y &amp;= \log_b (xy) \\
\log_b x - \log_b y &amp;= \log_b \left( \frac{x}{y} \right) \\
\log_b x^y &amp;= y \log x,
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;and many more, but these are the important ones for our purposes. The derivative of a logarithm is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{d}{dx} (\log_b x) = \frac{1}{x \ln b},&lt;/script&gt;

&lt;p&gt;which makes the derivative of the natural logarithm
&lt;span class=&quot;marginnote&quot;&gt;Since the natural logarithm is just a logarithm with base $e$, then $\frac{d}{dx} (\ln x) = \frac{d}{dx} (\log_e x) = \frac{1}{x \ln e}$, and since the natural logarithm $\ln (x)$ is defined as the inverse function of $e^x$, then we just end up with $\frac{1}{x \ln e} = \frac{1}{x}$.&lt;/span&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{d}{dx} (\ln x) = \frac{1}{x}.&lt;/script&gt;

&lt;h4 id=&quot;example-1&quot;&gt;Example:&lt;/h4&gt;
&lt;p&gt;Let’s try and find the derivative with respect to $z$ of $\ln \left( 17 + z^{3/2} \right)$. We notice that this function is composite, and the outer function is the natural logarithm with derivative $\frac{1}{x}$. Therefore, using the chain rule and the derivative of the natural logarithm, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{d}{dz} \ln \left( 17 + z^\frac{3}{2} \right)
&amp;= \frac{1}{17 + z^\frac{3}{2}} \frac{d}{dz} \left( 17 + z^\frac{3}{2} \right) \\
&amp;= \frac{1}{17 + z^\frac{3}{2}} \frac{3}{2}z^\frac{1}{2} \\
&amp;= \frac{3 \sqrt{z}}{34 + 2z^\frac{3}{2}}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;extrema&quot;&gt;Extrema&lt;/h3&gt;
&lt;p&gt;Extrema is the collective term for maxima and minima, which are the largest and smallest values of a function. Usually we distinguish between a &lt;strong&gt;local extremum&lt;/strong&gt; and a &lt;strong&gt;global extremum&lt;/strong&gt;, where local refers to a specific interval, and global refers to the domain of the function.&lt;/p&gt;

&lt;p&gt;A point $x$ is an local maximum or minimum of a function $f$ in the interval $\left[ a,b \right]$, if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) \geq f(x') \text{ or } f(x) \leq f(x') \text{ for all } x' \in \left[ a,b \right].&lt;/script&gt;

&lt;p&gt;While there are a couple of exceptions, we can usually find the extrema of a function by calculating its derivative and setting it equal to zero. This is because, as one might intuitively reason, the curve of the function “changes direction” at an extremum as illustrated below with the slope.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt;Notice how the slope (pink) of the function (purple) is $0$ (horizontal) when at an extrema.&lt;/span&gt;&lt;/p&gt;
&lt;figure&gt;
    &lt;video width=&quot;500&quot; height=&quot;310&quot; loop=&quot;&quot; muted=&quot;&quot; autoplay=&quot;&quot;&gt;
        &lt;source src=&quot;http://localhost:4000/extra/bslialo-notes-9b/fig_01.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;We can also think of the extrema of a function as the point where the derivative changes sign, i.e., before reaching a maximum the slope is positive, but after passing the maximum the slope is negative. Likewise, before reaching the minimum the slope is negative, but after passing the minimum the slope is positive. This is also known as the &lt;strong&gt;first derivative test&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;example-2&quot;&gt;Example:&lt;/h4&gt;
&lt;p&gt;Let’s try and find the extrema of $f(x) = x^4-6x^3+12x^2-8x$. The derivative can be factorized as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{df}{dx}
&amp;= \frac{d}{dx} \left( x^4-6x^3+12x^2-8x \right) \\
&amp;= 4x^3-18x^2+24x-8 \\
&amp;= 2(x-2)^2 (2x-1).
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Setting this equal to $0$ gives us two possible values of $x$: $2$ and $\frac{1}{2}$. To check if these points are extrema, we use the first derivative test and check the value of the derivative before and after each point. We see&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
f'(x) &amp;&lt; 0 \quad \text{for} \quad  x &lt; \frac{1}{2}, \\
f'(x) &amp;&gt; 0 \quad \text{for} \quad \frac{1}{2} &lt; x &lt; 2, \\
f'(x) &amp;&gt; 0 \quad \text{for} \quad 2 &lt; x.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;This implies that $f(x)$ has a minimum at $x=\frac{1}{2}$, since the slope is negative (below $0$) before $x=\frac{1}{2}$ and positive after. But something weird is happening at $x=2$. By plotting $f$ we can see, what goes on:&lt;/p&gt;

&lt;video width=&quot;500&quot; height=&quot;310&quot; loop=&quot;&quot; muted=&quot;&quot; autoplay=&quot;&quot;&gt;
    &lt;source src=&quot;http://localhost:4000/extra/bslialo-notes-9b/fig_03.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;

&lt;p&gt;We can interpret this as, if we’re approaching from the left, then $x=2$ is a maximum, but approaching from the right it’s a minimum. Therefore $x=2$ can &lt;strong&gt;neither be a maximum nor minimum&lt;/strong&gt;, and we call it a &lt;strong&gt;saddle point&lt;/strong&gt;.
&lt;span class=&quot;marginnote&quot;&gt;We call it a saddle point because in $3$ dimensions a saddle point looks like a saddle.&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;optimisation-example-putting-it-all-together&quot;&gt;Optimisation example (putting it all together)&lt;/h3&gt;
&lt;p&gt;Now, let’s take everything we’ve learned, put it together, and use it! Optimisation problems often aim to find the extrema of a specifically constructed function. In most literature this function is called the &lt;strong&gt;objective function&lt;/strong&gt;, and the aim is to find its maximum or minimum.
&lt;span class=&quot;marginnote&quot;&gt;The objective function can be interpreted as the error or accuracy of a model, and our goal is to minimize or maximize this measure, hence optimizing the model.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Suppose we have a coin, and we want to find the most likely probability (parameter) for this coin to hit heads and tails. That is, we want to create a model that calculates the most likely number of heads and tails in a number of coin tosses. If we let $\pi$ represent the unknown probability of heads, $1-\pi$ the unknown probabilty of tails, and toss the coin $100$ times, then our result could look something like this&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;HHTHTTHHHTHT \dots&lt;/script&gt;

&lt;p&gt;Let’s assume we got 46 heads and 54 tails. We write up the probability of the specific sequence we got (our data) given the unknown parameter $\pi$ as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
P( \text{data} | \text{parameter} )
&amp;= P(HHTHTTHHHTHT \dots | \pi) \\
&amp;= \pi \pi (1-\pi) \pi (1-\pi) (1-\pi) \pi \pi \pi (1-\pi) \pi (1-\pi) \dots \\
&amp;= \pi^{46} (1-\pi)^{54}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;The left-hand side is read as “the probability of the data given the parameter”. Since the probability of heads and tails is $\pi$ and $1-\pi$ respectively, and they’re independent events, we can take the product of the probabilities of each coin toss in our sequence to get the probability of the entire sequence&lt;span class=&quot;sidenote-number&quot;&gt;&lt;/span&gt;.
&lt;span class=&quot;sidenote&quot;&gt;This is commonly known as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Probability#Summary_of_probabilities&quot;&gt;product rule of probability&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;We want to figure out, what the most likely probability of heads is $(\pi)$ with our data of 46 heads and 54 tails. Just like our data, the parameter $\pi$ has a fixed value too, but we don’t know what it is. We therefore treat it as a variable between 0 and 1 that we are free to choose, which gives us a function of the parameter&lt;span class=&quot;sidenote-number&quot;&gt;&lt;/span&gt;
&lt;span class=&quot;sidenote&quot;&gt;This is the so-called &lt;a href=&quot;https://en.wikipedia.org/wiki/Likelihood_function&quot;&gt;likelihood function&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L} ( \text{parameter} | \text{data} ) = \pi^{46} (1-\pi)^{54}.&lt;/script&gt;

&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt;Plot of the likelihood function, where the x-axis is the value of our parameter $\pi$, which goes from $0$ to $1$, and the y-axis is the relative probability of that specific value of $\pi$ given our data. Don’t worry about the low values on the y-axis, since we’re only interested in the values compared to each other.&lt;/span&gt;&lt;/p&gt;
&lt;figure&gt;
    &lt;video width=&quot;500&quot; height=&quot;310&quot; loop=&quot;&quot; muted=&quot;&quot; autoplay=&quot;&quot;&gt;
        &lt;source src=&quot;http://localhost:4000/extra/bslialo-notes-9b/fig_02.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;The maximum value of the likelihood function is at $\pi=0.46$, and we’re done! We now have a parameter $\pi = 0.46$ that gives us the most likely ratio of heads to coin tosses, given the data we produced. We can think of this as optimizing our coin toss model, but let’s examine closer, what we did. We tossed a coin $100$ times, asked ourselves what the most likely probability of heads is, and called this value $\pi$. Then we wrote up the likelihood function of $\pi$, and found the value of $\pi$ corresponding to the maximum of the likelihood function.&lt;/p&gt;

&lt;p&gt;Let’s now try and generalize this method, in case we produced different data. Say we toss the coin $n$ times, and we get $k$ heads. Our likelihood function would be&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L} ( \pi | \text{data} ) = \pi^k (1-\pi)^{n-k}.&lt;/script&gt;

&lt;p&gt;Then we want to find the maximum of this function. While we could try and take the derivative straight away, there is an important theorem, we use instead: the natural logarithm is a strictly increasing function, so the natural logarithm of a function achieves its maxima at the same values as the function itself. Therefore, instead of finding the maximum of $\mathcal{L}$, we can find the maximum of $\ln ( \mathcal{L} )$ which often times is easier. By using the rules of logarithms, we find&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\ln ( \mathcal{L}( \pi | \text{data} ) )
&amp;= \ln \left( \pi^k (1-\pi)^{n-k} \right) \\
&amp;= \ln (\pi^k) + \ln \left( (1-\pi)^{n-k} \right) \\
&amp;= k \ln (\pi) + (n-k) \ln (1-\pi).
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now, we want to find the maximum of this function, which we do by finding the derivative and setting it equal to zero. We can use the chain rule and the derivative of logarithms to find the derivative of the likelihood function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{d}{d\pi} \ln ( \mathcal{L} )
&amp;= \frac{d}{d\pi} \left( k \ln (\pi) + (n-k) \ln (1-\pi) \right) \\
&amp;= \frac{k}{\pi} + (-1) \frac{n-k}{1-\pi} \\
&amp;= \frac{k}{\pi} - \frac{n-k}{1-\pi}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Then we set the derivative equal to zero and solve for $\pi$ to find the maximum&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\frac{d}{d\pi} \ln ( \mathcal{L} )
&amp;= 0 \\
\frac{k}{\pi} - \frac{n-k}{1-\pi}
&amp;= 0 \\
k(1-\pi) - \pi(n-k)
&amp;= 0 \\
k - k \pi - \pi n + \pi k
&amp;= 0 \\
k - \pi n
&amp;= 0 \\
\pi
&amp;= \frac{k}{n},
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;and we’re done! The most likely probability of heads is therefore the number of heads $k$, we get in $n$ coin tosses ($\frac{46}{100} = 0.46$ in our example).&lt;/p&gt;

&lt;p&gt;While the example at hand is simple enough to find the result without using our method, we can use this exact method in many other scenarios, where the results aren’t as easily found.&lt;/p&gt;
</description>
        <pubDate>Sun, 04 Nov 2018 00:00:00 -0400</pubDate>
        <link>http://localhost:4000//bslialo-notes-9b/</link>
        <guid isPermaLink="true">http://localhost:4000//bslialo-notes-9b/</guid>
      </item>
    
      <item>
        <title>BSLIALO, notes 9a: The derivative, the limit, and the concept of approach</title>
        <description>&lt;p&gt;Imagine you’re participating in a $100$ metres sprint race. When you hear the starter’s pistol, you accelerate for dozens of metres, i.e. your velocity will increase until you reach your maximum speed. Let’s assume, you reach your maximum speed $5$ seconds into the race, after which you start to fatigue and decelerate - and let’s also assume, you finish the race in $10$ seconds. If we let $s(t)$ be the distance you’ve traveled at time $t$, and $v(t)$ your velocity at time $t$, then we can draw your velocity and the distance you’ve traveled in a plot like the one below.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt;Notice how towards the middle of the race you’re traveling more metres per unit of time than at the beginning and end.&lt;/span&gt;&lt;/p&gt;
&lt;figure&gt;
    &lt;video width=&quot;500&quot; height=&quot;310&quot; loop=&quot;&quot; muted=&quot;&quot; autoplay=&quot;&quot;&gt;
        &lt;source src=&quot;http://localhost:4000/extra/bslialo-notes-9a/fig_01.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    &lt;/video&gt;
&lt;/figure&gt;

&lt;!--more--&gt;

&lt;p&gt;We know that to calculate velocity, we take the ratio of the distance traveled and the time it took. If we start at $s(t_1)$ and end at $s(t_2)$, we say&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_{\text{average}} = \frac{s(t_2)-s(t_1)}{t_2-t_1}.&lt;/script&gt;

&lt;p&gt;We can write this more formally as the average velocity from $t_1$ to some amount of time after that, if we use the time difference $t_2-t_1=\Delta t$. This gives us&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_{\text{average}} = \frac{s(t_1 + \Delta t)-s(t_1)}{\Delta t}. \quad \quad (1)&lt;/script&gt;

&lt;p&gt;This is a &lt;strong&gt;change&lt;/strong&gt; in distance divided by a &lt;strong&gt;change&lt;/strong&gt; in time, but if we were to ask, how fast you were running at an instant, the question doesn’t seem to make sense. Because no distance is traveled in a single point in time, and no time has passed in an instant. Yet, in the plot above we have a function $v(t)$ that seemingly gives us the velocity at any point in time. The function gives us the &lt;strong&gt;instantaneous rate of change&lt;/strong&gt; of distance.&lt;/p&gt;

&lt;h3 id=&quot;approach-and-limits&quot;&gt;Approach and limits&lt;/h3&gt;
&lt;p&gt;To understand this phenomenon of &lt;strong&gt;instantaneous change&lt;/strong&gt;, we have to talk about the concept of approach. Let’s look at the function $f(x) = x^2+1$. Imagine we had to figure out what $f(0)$ is equal to, without inserting $0$ into the function. One way of doing it could be to start from a number higher than $0$ (let’s call it $a$) and gradually evaluate the function while decreasing $a$. This would give us a sequence&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
f(1)&amp;=2, \quad &amp;\text{when } a&amp;=1 \\
f(0.5)&amp;=1.7, \quad &amp;\text{when } a&amp;=0.5 \\
f(0.25)&amp;=1.5, \quad &amp;\text{when } a&amp;=0.25 \\
f(0.125)&amp;=1.35, \quad &amp;\text{when } a&amp;=0.125,
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;and so on. Now, we know that $f(0)=0^2+1=1$, but let’s say that we didn’t know or were unable to do this calculation.
How would we go about showing that $f(0)=1$ without actually evaluating it?&lt;/p&gt;

&lt;p&gt;Well, if we can show for any value $c$ that we can get closer to $1$ for some value $x_c$, then that’s pretty good.
That is, if we’re given a value $c$ (close to $1$), we want to show that we can find a value $x_c$ that gets $f(x)$ closer to $1$ than $c$ is.
Formally, this means that for all possible values of $c$, we must be able to find a value $x_c$ that satisfies $\mid 1-f(x_c) \mid &amp;lt; \mid 1-c \mid $.
If we can be certain about this, then we can say that $f(x)$ approaches $1$ as $x$ approaches $x_c$&lt;span class=&quot;sidenote-number&quot;&gt;&lt;/span&gt;.
&lt;span class=&quot;sidenote&quot;&gt;This is not the &lt;a href=&quot;https://en.wikipedia.org/wiki/(%CE%B5,_%CE%B4)-definition_of_limit#Precise_statement_and_related_statements&quot;&gt;rigorous definition of a limit&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;We can certainly do this! Let’s say $c=1.01$, then we solve&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
f(x) &amp;= 1.01 \\
x^2 + 1 &amp;= 1.01 \\
x &amp;= \sqrt{1.01-1} \\
x &amp;= \pm 0.1
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;and choose $x_c$ to be between $-0.1$ and $0.1$. We can generalize this such that for a given $c$, we pick $x_c$ to be between $-\sqrt{c-1}$ and $\sqrt{c-1}$. The key thing here is, if we want to say that a function approaches a value, then we have to be able to get &lt;strong&gt;arbitrarily close&lt;/strong&gt; to it - and this has to be the case, if we’re approaching from either side of $x_c$ (higher or lower).&lt;/p&gt;

&lt;p&gt;This is the concept of approach and the intuition behind limits. Suppose $f(x)$ is defined for all $x$ around $x_c$ but not necessarily at $x_c$. We can say&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lim_{x \to x_c} f(x) = c, \quad \quad (2)&lt;/script&gt;

&lt;p&gt;if we can get $f(x)$ arbitrarily close to $c$ for values of $x$ close but not equal to $x_c$. Equation $(2)$ is read: “the limit of $f$ of $x$ as $x$ approaches $x_c$ equals $c$”. Another way to write this is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) \to c \quad \text{as} \quad x \to x_c,&lt;/script&gt;

&lt;p&gt;which is read: “$f$ of $x$ approaches $c$ as $x$ approaches $x_c$”.&lt;/p&gt;

&lt;h4 id=&quot;example&quot;&gt;Example:&lt;/h4&gt;
&lt;p&gt;Suppose we have to find the limit&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lim_{x \to 1} \frac{1-\sqrt{x}}{1-x}.&lt;/script&gt;

&lt;p&gt;The problem here is that we cannot insert $x=1$ into the fraction, as we would then divide by $0$. Instead, we do a bit of algebraic manipulation to get a limit, we can evaluate&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\lim_{x \to 1} \frac{1-\sqrt{x}}{1-x}
&amp;= \lim_{x \to 1} \frac{1-\sqrt{x}}{(1-\sqrt{x})(1+\sqrt{x})} \\
&amp;= \lim_{x \to 1} \frac{1}{1+\sqrt{x}} \\
&amp;= \frac{1}{1+\sqrt{1}} \\
&amp;= \frac{1}{2}.
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;derivatives&quot;&gt;Derivatives&lt;/h3&gt;
&lt;p&gt;If we return to our initial example of the sprint race, we asked ourselves: how fast were you running at a point in time? With our new knowledge of approach and limits, we can rephrase the question and instead ask: what happens when $\Delta t$ approaches $0$ in equation $(1)$? The question asks us to find&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v(t) = \lim_{\Delta t \to 0} \frac{s(t+\Delta t)-s(t)}{\Delta t}. \quad \quad (3)&lt;/script&gt;

&lt;p&gt;&lt;span class=&quot;marginnote&quot;&gt;The animation illustrates how $v(t)$ changes as $\Delta t$ approaches $0$.&lt;/span&gt;&lt;/p&gt;
&lt;figure&gt;
    &lt;video width=&quot;500&quot; height=&quot;310&quot; loop=&quot;&quot; muted=&quot;&quot; autoplay=&quot;&quot;&gt;
        &lt;source src=&quot;http://localhost:4000/extra/bslialo-notes-9a/fig_02.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;As can be seen above, the velocity turns out to be the slope or tangent line of the distance. This is also what we call the derivative of the distance function. This is usually denoted&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{d}{dt}s(t) = v(t),&lt;/script&gt;

&lt;p&gt;where $\frac{d}{dt}$ is the differentiation operator.
&lt;span class=&quot;marginnote&quot;&gt;Differentiation is the process of finding a derivative.&lt;/span&gt;
Using the common notation $y=f(x)$ to show that the independent (free) variable is $x$ and the dependent variable is $y$, we can write up several notations for the derivative:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{df}{dx} = \frac{dy}{dx} = \frac{d}{dx}f(x) = f'(x) = y' = Df(x) = D_x f(x).&lt;/script&gt;

&lt;h4 id=&quot;example-1&quot;&gt;Example:&lt;/h4&gt;
&lt;p&gt;Let $f(x) = x + \frac{1}{x}$. Let’s try to find the derivative using the definition from $(3)$. We see&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
f'(x)
&amp;= \lim_{\Delta x \to 0} \frac{f(x+\Delta x)-f(x)}{\Delta x} \\
&amp;= \lim_{\Delta x \to 0} \frac{x+\Delta x + \frac{1}{x + \Delta x} - \left( x + \frac{1}{x} \right)}{\Delta x} \\
&amp;= \lim_{\Delta x \to 0} \frac{\Delta x + \frac{1}{x + \Delta x} - \frac{1}{x}}{\Delta x} \\
&amp;= \lim_{\Delta x \to 0} 1 + \frac{x - (x + \Delta x)}{\Delta x (x + \Delta x)x} \\
&amp;= \lim_{\Delta x \to 0} 1 - \frac{1}{(x + \Delta x)x} \\
&amp;= 1 - \frac{1}{x^2},
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;which is the actual derivative.&lt;/p&gt;

&lt;h3 id=&quot;final-thoughts&quot;&gt;Final thoughts&lt;/h3&gt;
&lt;p&gt;One of my professors once said: “differentiation is mechanics and integration is art”. At first it just sounded like a crap quote, but it’s more of a general saying than a quote really. Differentiation is mostly tedious calculation, and you just have to practice it a lot to learn it. However, integration often requires some creativity, but we’ll get to that later on. I’m considering uploading a few exercises after this first week of calculus that try and teach some core tips and tricks of differentiation and integration.&lt;/p&gt;

</description>
        <pubDate>Thu, 25 Oct 2018 00:00:00 -0400</pubDate>
        <link>http://localhost:4000//bslialo-notes-9a/</link>
        <guid isPermaLink="true">http://localhost:4000//bslialo-notes-9a/</guid>
      </item>
    
  </channel>
</rss>
